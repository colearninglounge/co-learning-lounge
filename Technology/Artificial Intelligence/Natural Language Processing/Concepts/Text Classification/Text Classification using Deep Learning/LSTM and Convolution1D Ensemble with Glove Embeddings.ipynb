{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bOYONMdXL-6"
   },
   "source": [
    "Text classification is one of the major problems people are now days looking to solve on in this field. Imagine your boss coming up to you assigning a task to classify 1 Million emails into different categories. The first thought that comes to your mind is that who will read so many emails and classify it manually.\n",
    "\n",
    "But wait as a Data Scientist this is where your skills come into the picture. Why not use NLP skills in this problem to solve it and save a hell lot of time.\n",
    "\n",
    "So are you ready to learn word embeddings and get your hands dirty with some coding?\n",
    "\n",
    "If you are new to Text classification and Machine learning then you should read [this](https://github.com/colearninglounge/co-learning-lounge/tree/master/Technology/Artificial%20Intelligence/Natural%20Language%20Processing/Concepts/Text%20Classification/Text%20Classification%20using%20Machine%20Learning) tutorial first.\n",
    "\n",
    "Post your doubt/feedback/discussion in our FB group unit [here](https://www.facebook.com/groups/colearninglounge/learning_content/?filter=471702823590059) in the appropriate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvAFFfYtXL-7"
   },
   "source": [
    "## Table of content:\n",
    "    1)  Introduction\n",
    "    2)  Import library\n",
    "    3)  Load dataset\n",
    "    4)  Extracting values from the Embeddings Text File\n",
    "    5)  Adding words not present in Embedding File\n",
    "    6)  Preprocessing:\n",
    "            Tokenization\n",
    "            Padding\n",
    "    7)  Creating Embedding Matrix\n",
    "    8)  Keras Model:\n",
    "            Creation\n",
    "            Compilation\n",
    "            Training\n",
    "            Evaluation\n",
    "    9)  Loading Testing Data\n",
    "    10) Preprocessing:\n",
    "            Tokenization\n",
    "            Padding\n",
    "    11) Prediction\n",
    "    12) Combining Different Models to Create Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "of4CS2ukXL-8"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial we will working with Text Classification Data i.e. categorizing given text into multiple categories. Steps which we will be following are converting the sentences into tokens (Individual words). Then we will be converting the words into numbers such that every sentence to be classified will be represented by an array of numbers, followed by padding of the sentences so that all observations are of equal length. When we are done with preparing of our data we will use Glove Embeddings and using Keras framework for Deep Learning. Glove Embeddings are pre defined embeddings that is most of the words used in english are represented as a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQpDnb0EXL-9"
   },
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "dUkG2d0AXL-9",
    "outputId": "a6ec7eb5-db42-4878-92e2-a55b6c80b337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jigsaw-toxic-comment-classification-challenge', 'gloveembeddings']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kU3ZmsYPXL_C",
    "outputId": "a2297025-bf67-400f-e4bd-eba5e0012397"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,GlobalMaxPool1D,Dropout,Embedding,Bidirectional,Flatten,CuDNNLSTM,Convolution1D,MaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ljChz6T9XL_G"
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Dataset can be accessed from kaggle [here](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview)\n",
    "Glove Embeddings can be downloaded from [here](https://nlp.stanford.edu/projects/glove/). Download glove.6B.zip. It contains different embeddings with lengths varing from 50,100,200 and 300 dimensions. These different files with different embeddings can be used for different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "26R4fWkrXL_H"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWzMn-t8XL_J",
    "outputId": "e9d57372-a8a1-4e79-89e4-d6d41c20e8cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id      ...      identity_hate\n",
       "0  0000997932d777bf      ...                  0\n",
       "1  000103f0d9cfb60f      ...                  0\n",
       "2  000113f07ec002fd      ...                  0\n",
       "3  0001b41b1c6bb37e      ...                  0\n",
       "4  0001d958c54c6e35      ...                  0\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nhy1pgV6XL_M"
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = df[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rb9NT5oZXL_O"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSj1bqn3XL_Q"
   },
   "source": [
    "Using word embeddings so that words with similar words have similar representation in vector space. It represents every word as a vector. The words which have similar meaning are place close to each other. Quick understanding can be done from [this](https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Raf4DhrcXL_Q",
    "outputId": "82e31318-44ee-4d71-de08-91da697e5d1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:06, 66548.73it/s]\n"
     ]
    }
   ],
   "source": [
    "f = open('../input/gloveembeddings/glove.6B.50d.txt')\n",
    "embedding_values = {}\n",
    "for line in tqdm(f):\n",
    "    value = line.split(' ')\n",
    "    word = value[0]\n",
    "    coef = np.array(value[1:],dtype = 'float32')\n",
    "    embedding_values[word] = coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vo9s6lhnXL_U"
   },
   "source": [
    "There are many words in the training data which are not there in the glove embeddings. So we take the mean of the embeddings and replace the absent words in the embeddings with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wjNzVDaXL_U",
    "outputId": "afd9fc3b-c011-4feb-c045-ca567688371f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.020940498, 0.6441043)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embs = np.stack(embedding_values.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "emb_mean,emb_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWyrfXENXL_Z"
   },
   "outputs": [],
   "source": [
    "x = df['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7saajECXL_b"
   },
   "source": [
    "These are the steps that needs to be performed so that we can convert each word of our vocabulary into a unique integer. Tokenizer is initalized in first step. Then fitting on the text will help us create a vocabulary so that each word is assigned with a unique integer. Then we convert in the whole sentence of the comment into a sequence of numbers which are assigned by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqlij5SyXL_b"
   },
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQJp1PlWXL_d"
   },
   "outputs": [],
   "source": [
    "token.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpRLUXYZXL_g"
   },
   "outputs": [],
   "source": [
    "seq = token.texts_to_sequences(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xfi9sgaYXL_i"
   },
   "source": [
    "Padding the sequence helps in making all the sentence of same length. maxlen is the parameter which decides the length we want to assign to all the sentences. Padding is done by adding 0 on either the end of sentence or prior the sentence if the sentence is having length less than max length. This is also a parameter which user can change, by defaults its prefix. If the length of the sentence is more than 100 then it is pruned which brings down the length to 50 (maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYxlGkq8XL_i"
   },
   "outputs": [],
   "source": [
    "pad_seq = pad_sequences(seq,maxlen=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPcInobpXL_k"
   },
   "source": [
    "Now we will be converting each word in our vocabulary into word embeddings. This embedding is vector of 1x50 dimension which represents each word as a vector and placing them into a vector space. Embedding matrix is created in which the number assigned to the word by tokenizer is assigned with the corresponding vector which we get from the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-rSGULagXL_l",
    "outputId": "e54d897e-8754-4226-ed84-aeb095c94aa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210338\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(token.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIIC6rpaXL_o",
    "outputId": "0cb5d1bc-2c83-4bc5-c424-bc86788f13dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210337/210337 [00:00<00:00, 601491.58it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, 50))\n",
    "for word,i in tqdm(token.word_index.items()):\n",
    "    values = embedding_values.get(word)\n",
    "    if values is not None:\n",
    "        embedding_matrix[i] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FDLnm33PXL_t"
   },
   "source": [
    "Want to learn more about LSTM and RNN. Please follow [this link](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) for clear explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jV5NqEa6XL_u"
   },
   "source": [
    "New to Deep Learning?\n",
    "You can check out [this](https://medium.com/datadriveninvestor/making-your-first-neural-network-part-1-a683b80b93e3) article to get basic idea of how to build a simple deep learning model with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrLjqtxhXL_v"
   },
   "outputs": [],
   "source": [
    "model1 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8uUpgAMXL_y"
   },
   "source": [
    "Here this embedding layer is important as this will help us in training of the sentences with their respective embeddings whihc we have assigned above. The first parameter is the size of our vocabulary. Second parameter is the output embeddings length which is 50 in this case as we used the 50 glove embeddings of each word. The length of each observation which is expected by the network is given by input_length parameter. We have padded all the observations to 50 hence we set input_length = 50. Weights parameter shows that the embeddings which we want to use is embeddings_matrix and it should not be altered hence trainable is kept false. If we want to train our own embeddings we can simply remove the weights and trainable parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9WUbXYdtXL_0",
    "outputId": "b7a5d75b-f0b6-4363-ac89-0e25d8eefe92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model1.add(Embedding(vocab_size,50,input_length=50,weights = [embedding_matrix],trainable = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9EmuNf2XL_2"
   },
   "source": [
    "Building a LSTM model. LSTM networks are useful in sequence data as they are capable of remembering the past words which help them in understanding the meaning of the sentence which helps in text classification. Bidirectional Layer is helpful as it helps in understanding thesentence from start to end and also from end to start. It works in both the direction. This is useful as the reverse order LSTM layer is capable of learning patterns which are not possible for the normal LSTM layers which goes from start to end of the sentence in the normal order. Hence Bidirectional layers are useful in text classification problems as different patterns can be captured from 2 directions. CuDNNLSTM is same as LSTM. If you are using GPU then CuDNNLSTM will be faster but if you are using CPU please use LSTM.\n",
    "There are serveral pooling techniques used. Average Pooling, Global Average Pooling, Max Pooling and Global Max Pooling. Average Pooling and Max Pooling take in kernel size as argument. If the input of the max pooling layer is 0,1,2,2,5,1,2, global max pooling outputs 5, whereas ordinary max pooling layer with pool size equals to 3 outputs 2,2,5,5,5 (assuming stride=1). Same is the case with Average Pooling only instead of taking the maximum values we do the average of all the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B_E7m_XXL_3"
   },
   "outputs": [],
   "source": [
    "model1.add(Bidirectional(CuDNNLSTM(50,return_sequences=True)))\n",
    "model1.add(GlobalMaxPool1D())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N2enNfafXL_5"
   },
   "source": [
    "Dense Layers is a fully connencted layer that is each input node in one layer is connected to all the neurons in the next layer. It is basically helping us out in classification of the dependent variable with an activation function which tells the neuron when to be activated. Relu activation function is used here. There are many different activation functions which can be used like sigmoid, tanh, relu etc. Dropout layers is helping us to avoid overfitting. In dropout few neurons are randomly turned off. This is done in order to remove the dependency of neurons on each other. No neuron is particularly responsible for learning a specific feature. The argument 0.2 specifies that 20% of the neurons in this layers are going to the turned off. This values ideally ranges between 0.1 to 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaSnfzRfXL_5"
   },
   "source": [
    "There are several activation functions which can be used in there layers. Relu is used in many cases as it out performs many other activation functions. You can try tuning the model with different activation functions such as sigmoid, tanh etc. More detailed information on activation functions can be found [here](https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GFYAvjndXL_6",
    "outputId": "22da7173-d9c3-4218-9305-5799efdaa73d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model1.add(Dense(50,activation = 'relu'))\n",
    "model1.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PE8ga-NzXL_9"
   },
   "source": [
    "The last layers is the dense layer with number of neurons equal to number of classes of dependent variable (6 in this case). A dense layer is just a regular layer of neurons in a neural network. Each neuron recieves input from all the neurons in the previous layer, thus densely connected. The layer has a weight matrix W, a bias vector b, and the activations of previous layer a. The following is te docstring of class Dense from the keras documentation: output = activation(dot(input, kernel)+ bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gjNGxRkjXL_-"
   },
   "source": [
    "The activation used here is sigmoid. We use sigmoid because its a multi label classification problem (one observation can belong to more than one class). This is one of the special cases but when we are working on a multi label classification problem where one observation is assigned to a single class then we use softmax activation function in our final layer. It performs same as a sigmoid function that is it provides us with the probabilities of the observation belonging to a particular class. We pick the maximum probability and assign that observation to the class having maximum probability. Sigmoid is used when we are dealing with a binary classification problem. Probability less than 0.5 is assigned to 0 class and more than 0.5 is assigned to 1 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGbyL-pQXL__"
   },
   "outputs": [],
   "source": [
    "model1.add(Dense(6,activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6wiQP9aUXMAA"
   },
   "source": [
    "Compiling the model is final step to complete building the model. Optimizer is like the cost function (similar to gradient descent). Adam is one of the best performing function used in deep learning models. We can change and test with different optimizers too. Mathimatical intuation of all the optimizers can be found [here](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f). \n",
    "Binary cross entropy is the loss function which checks the loss in predictions made by the model by calulating the distance between the predicted value and the actual value. We use categorical_crossentropy when dealing with multiclass classification problem and binary_crossentropy when dealing with binary classification. This is a special case for multilabel classification problem hence we are looking for probability of observation belonging to each class. Top few classes are taken for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAMfRjPHXMAB"
   },
   "source": [
    "Accuracy is being used for checking the performance of the model. Accuracy is one the evalution which can be used check if the model is overfitting or underfitting. If the training accuracy is very high as compared to testing accuracy we can see that the model is overfitting as it is not generalizing well on the unseen data (test data). To avoid overfitting we can reduce the complexity of the model, add more data, add more regularization (dropout, batch normalization). If the training accuracy is much less than testing accuracy then the model is underfitting which means model is not able to capture the features so we need to train the model with more number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_PoyVV-XMAB"
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPTrMeMeXMAD"
   },
   "source": [
    "Batch size is the parameter which defines how many oberservations are fed into the model at a time for training. More the batch size more computational resources and memory is required for processing. This is one of the hyperparameter which can be used for tuning the model. Batch Size of 1 will fed one observation at a time which is really useful but the training and convergence speed will be very slow and model might overfit easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HZ5g_JcXMAE",
    "outputId": "1c7dfca6-8778-43fb-9317-62f6c7159f16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 43s 303us/step - loss: 0.0684 - acc: 0.9773 - val_loss: 0.0583 - val_acc: 0.9800\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 41s 284us/step - loss: 0.0551 - acc: 0.9808 - val_loss: 0.0542 - val_acc: 0.9812\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 41s 284us/step - loss: 0.0516 - acc: 0.9818 - val_loss: 0.0527 - val_acc: 0.9816\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(pad_seq,y,epochs =3,batch_size=32,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jfq8waDrXMAH",
    "outputId": "ec33bf5b-954b-46fc-9ca5-bdecf29321b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            10516900  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 50, 100)           40800     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 10,563,056\n",
      "Trainable params: 46,156\n",
      "Non-trainable params: 10,516,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaW0zhPXXMAK"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pAev7sWhXMAN",
    "outputId": "e81a7ae1-cdfb-4670-eac3-08306280e89f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGsff0g9XMAP"
   },
   "source": [
    "Preprocessing of the test data so that model can easily make its prediction as it should be in the same format as that of our training data. Note that we are using the same tokenizer in our testing and we are not fitting it again because this might change the numbers assigned to words which are there in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sugDGWuBXMAQ"
   },
   "outputs": [],
   "source": [
    "x_test = test['comment_text']\n",
    "test_seq = token.texts_to_sequences(x_test)\n",
    "test_pad_seq = pad_sequences(test_seq,maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1zk-WlSXMAR"
   },
   "outputs": [],
   "source": [
    "predict1 = model1.predict(test_pad_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZirf1idXMAX"
   },
   "source": [
    "Here we are training with Simple LSTM without any Bidirectional Layer. If you want to add dropout for the LSTM layer then it should be done within the LSTM. Dropout argument is done for dropping inputs to the LSTM cell and recurrent dropout is done for dropping the long term memory of the LSTM connection. More clear explaination can be found [here](https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y26VjOT6XMAY"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size,50,input_length=50,weights = [embedding_matrix],trainable = False))\n",
    "model2.add(LSTM(50,dropout=0.1,recurrent_dropout=0.1))\n",
    "model2.add(Dense(50,activation = 'relu'))\n",
    "model2.add(Dense(6,activation = 'sigmoid'))\n",
    "model2.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBCuGP1wXMAb",
    "outputId": "e8143a7c-995e-4a98-e3c0-2286533d45a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 374s 3ms/step - loss: 0.0747 - acc: 0.9755 - val_loss: 0.0637 - val_acc: 0.9785\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 371s 3ms/step - loss: 0.0604 - acc: 0.9791 - val_loss: 0.0591 - val_acc: 0.9797\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 370s 3ms/step - loss: 0.0568 - acc: 0.9801 - val_loss: 0.0570 - val_acc: 0.9806\n"
     ]
    }
   ],
   "source": [
    "history = model2.fit(pad_seq,y,epochs =3,batch_size=32,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GiK1vDg7XMAh"
   },
   "outputs": [],
   "source": [
    "predict2 = model2.predict(test_pad_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMLqNZumXMAi"
   },
   "source": [
    "Here we will be using convolution1D model. This model works with a kernel of size 5 which hovers over the sentence capturing only the important features of the model. This will help in extracting the features that are significant to classify the text into different categories. After which we use a MaxPooling layer which will help in taking the maximum value from the kernel size of 2. This will also help us in reducing the computation and extracting only the important feature present under the kernel at that point of time. We can still change the model by changing the number of neurons and also by changing the number of layers which will change the computational complexity of the model too. Number of features to be extracted and kernel size are the hyperparameters you can tune to explore different models and results. More explaination on Convolution1D can be found [here](https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pISPJbOXMAj"
   },
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(vocab_size,50,input_length=50,weights = [embedding_matrix],trainable = False))\n",
    "model3.add(Convolution1D(9,kernel_size=5,activation='relu'))\n",
    "model3.add(MaxPool1D(2))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(50,activation = 'relu'))\n",
    "model3.add(Dense(6,activation = 'sigmoid'))\n",
    "model3.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THaLuX_UXMAk",
    "outputId": "0138fd53-82b4-4d18-b648-10445a2dfbc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 18s 123us/step - loss: 0.0877 - acc: 0.9706 - val_loss: 0.0773 - val_acc: 0.9742\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 17s 119us/step - loss: 0.0698 - acc: 0.9762 - val_loss: 0.0713 - val_acc: 0.9762\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 17s 119us/step - loss: 0.0654 - acc: 0.9776 - val_loss: 0.0687 - val_acc: 0.9765\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(pad_seq,y,epochs =3,batch_size=32,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uj4pcHr1XMAs"
   },
   "outputs": [],
   "source": [
    "predict3 = model3.predict(test_pad_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s-BalekQXMAu"
   },
   "source": [
    "Here we again use convolution1D but changing the number of filters and also the kernel size. Also instead of GlobalMaxPooling we are using MaxPooling1D with a Kernel size of 2 which means that a kernel with size 2 will hover the features and considering only the Maximum value under that kernel and discarding the other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMm0PlE_XMAu"
   },
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(vocab_size,50,input_length=50,weights = [embedding_matrix],trainable = False))\n",
    "model4.add(Convolution1D(18,kernel_size=3,activation='relu'))\n",
    "model4.add(MaxPool1D(2))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(50,activation = 'relu'))\n",
    "model4.add(Dense(6,activation = 'sigmoid'))\n",
    "model4.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOUuto6HXMAx",
    "outputId": "e49dbde5-299b-4e9a-f435-451026122cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/3\n",
      "143613/143613 [==============================] - 18s 125us/step - loss: 0.0835 - acc: 0.9721 - val_loss: 0.0719 - val_acc: 0.9759\n",
      "Epoch 2/3\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0667 - acc: 0.9771 - val_loss: 0.0697 - val_acc: 0.9766\n",
      "Epoch 3/3\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0626 - acc: 0.9783 - val_loss: 0.0668 - val_acc: 0.9776\n"
     ]
    }
   ],
   "source": [
    "history = model4.fit(pad_seq,y,epochs =3,batch_size=32,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4iojBxHXMA0"
   },
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl6lfJFzXMA1"
   },
   "source": [
    "Making prediction with Ensemble model as well where we can combine all the 4 models that we have trained. This helps in reducing the variance of the model and we can also say that more number of combined models are always better than a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OLFZt4p7XMA1"
   },
   "outputs": [],
   "source": [
    "ensemble_prediction = 0.25*predict1+0.25*predict2+0.25*predict3+0.25*predict4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRtcFjbvuYic"
   },
   "source": [
    "### Future Scope\n",
    "\n",
    "Changing the models with hyperparameter tuning and addition of layers can be done in order to increase the accuracy. Different embeddings can also be used instead of Glove Embeddings like word2vec or fastText. You can also use pre-trained models like BERT or use of attention layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This tutorial is intended to be a public resource. As such, if you see any glaring inaccuracies or if a critical topic is missing, please feel free to point it out or (preferably) submit a pull request to improve the tutorial. Also, we are always looking to improve the scope of this article. For anything feel free to mail us @ colearninglounge@gmail.com OR share your doubt/feedback in our FB group unit [here](https://www.facebook.com/groups/colearninglounge/learning_content/?filter=471702823590059) in the appropriate section.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YQl3HZBXMA3"
   },
   "source": [
    "\n",
    "\n",
    "### Author of this article is Samarth Sarin. You can contact him on [LinkedIn](https://www.linkedin.com/in/samarthsarin) or [Kaggle](https://www.kaggle.com/samarthsarin) for any doubts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2D0Fv2vuHcF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM and Convolution1D Ensemble with Glove.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
