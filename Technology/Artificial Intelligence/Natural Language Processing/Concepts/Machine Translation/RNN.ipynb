{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlHmnq6JbXJ7",
        "colab_type": "text"
      },
      "source": [
        "# RNN, LSTM, GRU - The Recurrent Family"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80lKqN4FX4th",
        "colab_type": "text"
      },
      "source": [
        "There are many resources available online explaining the structure, advantages, disadvantages and many other ascepts of RNN and it's variants. I can't explain any better than that. So here my focus will be on how to use RNN's and variants in PyTorch and also understanding the inputs, outputs of single layer, multi-layer, uni-directional and bi-directional RNN's. I hope by the end of this notebook you will be confident with using RNN's.\n",
        "\n",
        "![rnn](https://drive.google.com/uc?id=1S3sqnBDq0lX2EjqzKaHr5PQWaQeBPWdk)\n",
        "\n",
        "For those who are not aware of RNN's, I highly recommend to go through the following resources:\n",
        "\n",
        "- [Colah blog on LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [Edwin Chen blog on exploring LSTMs](http://blog.echen.me/)\n",
        "- [Illustrated guide to LSTMs and GRUs](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "- [Pytorch RNN code](https://pytorch.org/docs/stable/nn.html#recurrent-layers)\n",
        "\n",
        "\n",
        "If you feel anything wrong (or) any suggestions/feedback, please raise an issue [here](https://github.com/graviraja/100-Days-of-NLP/issues)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4jx7ROpgVXj",
        "colab_type": "text"
      },
      "source": [
        "We will divide this notebook into 4 sections.\n",
        "- Single layer, uni-directional RNN\n",
        "- Multi layer, uni-directional RNN\n",
        "- Single layer, bi-directional RNN\n",
        "- Multi layer, bi-directional RNN\n",
        "\n",
        "We will be using `PyTorch` for coding purposes.\n",
        "\n",
        "Before going into each of the section, let's first see the basic equations of RNN.\n",
        "\n",
        "The input to the RNN is a Sequence $X = \\{x_1, x_2,...., x_t\\}$ and the hidden states, $$H = \\{h_1, h_2,...., h_t\\}$$ are calcualted using the following equation:\n",
        "\n",
        "$$h_t = RNN(x_t, h_{t-1})$$\n",
        "\n",
        "![rnn](https://drive.google.com/uc?id=14Y-Eyzp5B0Fk8ol0T1OZ6O5pCNfHlt1H)\n",
        "\n",
        "In general, the outputs $$O = \\{o_1, o_2,.....,o_t \\}$$ are calculated using the following equation:\n",
        "\n",
        "$$o_t = ReLU(Linear(h_t))$$\n",
        "\n",
        "**Note: There is a small change in implementation of RNN when using PyTorch. Output is not calculated through the linear and relu functions, it is the same $h_t$ that means** **$$o_t = h_t$$**\n",
        "\n",
        "\n",
        "### **RNN is a single cell. Don't get confused by the unwrapped structure. The Weights of the network $U, W$ is same across all the unwrapped units.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozZHFPOghFpl",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "Letâ€™s create some dummy data, which can be used for understanding the above mentioned sections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8hEJ8UiXtQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5225d6f-ad73-4f25-b000-094855b83853"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch import LongTensor\n",
        "\n",
        "# create a dummy data of batch_size = 3\n",
        "data = ['long_str', 'tiny', 'medium']\n",
        "\n",
        "# create the vocabulary\n",
        "vocab = ['<pad>'] + sorted(set([char for seq in data for char in seq]))\n",
        "# vocab = ['<pad>', '_', 'd', 'e', 'g', 'i', 'l', 'm', 'n', 'o', 'r', 's', 't', 'u', 'y']\n",
        "\n",
        "# convert into numerical form\n",
        "vectorized_data = [[vocab.index(tok) for tok in seq] for seq in data]\n",
        "# vectorized_data = [[6, 9, 8, 4, 1, 11, 12, 10], [12, 5, 8, 14], [7, 3, 2, 5, 13, 7]]\n",
        "\n",
        "# prepare data, by padding with 0 (<pad> token), making the batch equal lengths\n",
        "seq_lengths = LongTensor([len(seq) for seq in vectorized_data])\n",
        "sequence_tensor = Variable(torch.zeros(len(vectorized_data), seq_lengths.max(), dtype=torch.long))\n",
        "\n",
        "for idx, (seq, seq_len) in enumerate(zip(vectorized_data, seq_lengths)):\n",
        "    sequence_tensor[idx, :seq_len] = LongTensor(seq)\n",
        "\n",
        "# sequence_tensor = ([[ 6,  9,  8,  4,  1, 11, 12, 10],\n",
        "#                     [12,  5,  8, 14,  0,  0,  0,  0],\n",
        "#                     [ 7,  3,  2,  5, 13,  7,  0,  0]])\n",
        "\n",
        "# convert the input into time major format\n",
        "sequence_tensor = sequence_tensor.t()\n",
        "# sequence_tensor shape => [max_len, batch_size]\n",
        "\n",
        "input_dim = len(vocab)\n",
        "print(f\"Length of vocab : {input_dim}\")\n",
        "\n",
        "# hidden dimension in the RNN\n",
        "hidden_dim = 5\n",
        "\n",
        "# embedding dimension\n",
        "embedding_dim = 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of vocab : 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6O6M6Fnh419",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer Uni-Directional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeI6o39Vh_1S",
        "colab_type": "text"
      },
      "source": [
        "![single layer uni dir rnn](https://drive.google.com/uc?id=1xgIOVPX0B-dP450pNIlIbRFSuCutbRa4)\n",
        "\n",
        "The input $X = \\{x_1, x_2, x_3, x_4\\}$ is passed through the RNN, and the outputs and hidden states are calcualted using the above equations.\n",
        "\n",
        "When passed the RNN, it returns output for each time step i.e $$\\{o_1^1, o_2^1, o_3^1, o_4^1\\}$$ and the final hidden state i.e $$\\{h_4^1\\}$$\n",
        "\n",
        "### Relation\n",
        "\n",
        "*The relation between the outputs and hidden state returned by the RNN is the final output is same as the final hidden state. \n",
        "$$o_4^1 == h_4^1$$*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UprEHNmRirgt",
        "colab_type": "text"
      },
      "source": [
        "### Implementation\n",
        "\n",
        "Considering the time-major format, shape of the inputs and outputs of RNN are as follows:\n",
        "\n",
        "> Input shape: [max_len, batch_size]\n",
        "\n",
        "> Output shape: [max_len, batch_size, hidden_size]\n",
        "\n",
        "> Hidden shape: [1, batch_size, hidden_size]\n",
        "\n",
        "Create the single layer, unidirectional RNN class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWlfImHPh9mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Single_Layer_Uni_Directional_RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input shape => [max_len, batch_size]\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        # embed shape => [max_len, batch_size, embedding_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embed)\n",
        "        # output shape => [max_len, batch_size, hidden_size]\n",
        "        # hidden shape => [1, batch_size, hidden_size]\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzYLXUX0i1bU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "daefc786-a02d-42ba-a191-25ab39c317e4"
      },
      "source": [
        "n_layers = 1\n",
        "bidirectional = False\n",
        "model = Single_Layer_Uni_Directional_RNN(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, hidden = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 5])\n",
            "Hidden shape is : torch.Size([1, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0paDSpsi83f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :] == hidden[0]).all(), \"Final output must be same as Hidden state in case of Single layer uni-directional RNN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3V94waRjAPC",
        "colab_type": "text"
      },
      "source": [
        "## Multi Layer, Uni-Directional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOJo3QhQjB39",
        "colab_type": "text"
      },
      "source": [
        "![multi uni direc rnn](https://drive.google.com/uc?id=1VX-h8DrNfWE9JucUE9u-5xULhdSLAJUm)\n",
        "\n",
        "Let's consider a 2 layer RNN, and the concept is same for more layers.\n",
        "\n",
        "The input $X = \\{x_1, x_2, x_3, x_4\\}$ is passed through the first layer of RNN, and the outputs of first layer are then passed as the inputs to the second layer RNN.\n",
        "\n",
        "The outputs returned are the outputs of final layer of RNN. \n",
        "\n",
        "The hidden states are the final hidden state of each layer in RNN.\n",
        "\n",
        "So the outputs are  $$\\{o_1^2, o_2^2, o_3^2, o_4^2\\}$$ and the final hidden state i.e $$\\{h_4^1, h_4^2\\}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsLeLbSEjqaA",
        "colab_type": "text"
      },
      "source": [
        "### Relation\n",
        "\n",
        "*The relation between the outputs and hidden state returned by the RNN is the final output is same as the final hidden state of the final layer. i.e $$o_4^2 == h_4^2$$*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQunB6yajtZ7",
        "colab_type": "text"
      },
      "source": [
        "### Implementation\n",
        "\n",
        "Considering the time-major format, shape of the inputs and outputs of RNN are as follows:\n",
        "\n",
        "> Input shape: [max_len, batch_size]\n",
        "\n",
        ">Output shape: [max_len, batch_size, hidden_size]\n",
        "\n",
        ">Hidden shape: [num_layers, batch_size, hidden_size]\n",
        "\n",
        "Create the multi layer, unidirectional RNN class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RWkKRHLjg24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Multi_Layer_Uni_Directional_RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input shape => [max_len, batch_size]\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        # embed shape => [max_len, batch_size, embedding_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embed)\n",
        "        # output shape => [max_len, batch_size, hidden_size]\n",
        "        # hidden shape => [num_layers, batch_size, hidden_size]\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFcrGf2Ij16N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3736aba1-446f-471f-cbb8-b28a0fbc1c63"
      },
      "source": [
        "n_layers = 2\n",
        "bidirectional = False\n",
        "model = Multi_Layer_Uni_Directional_RNN(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, hidden = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 5])\n",
            "Hidden shape is : torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUKSU9uhj36q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :] == hidden[-1]).all(), \"Final output must be same as Final Hidden state in case of Multi layer uni-directional RNN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njuc4v8jj6TU",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer, Bi-Directional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASVOCt-pj9Lg",
        "colab_type": "text"
      },
      "source": [
        "![single layer bi-directional rnn](https://drive.google.com/uc?id=1RYH58M4-FVcaly3A_4m59XpQwoJ5KKIj)\n",
        "\n",
        "Same as before, the input to the RNN is $X = \\{x_1, x_2, x_3, x_4\\}$\n",
        "The difference here is that, there are 2 RNN's. We call them Forward RNN (which reads input from left to right $$x_1 ... x_4$$  and Backward RNN which reads input from right to left $$x_4 ... x_1$$ \n",
        "\n",
        "Corresponding to 2 RNN's there will be 2 outputs (forward and backward) and 2 hidden outputs (forward and backward)\n",
        "\n",
        "\n",
        "**Note: The naming convention of hidden states and outputs varies from place to place.**\n",
        "\n",
        "- *We denote the hidden state $\\overrightarrow{h_t}$ after reading the input $x_t$ from left to right*\n",
        "- *We denote the hidden state $\\overleftarrow{h_t}$ after reading the input $x_t$ from right to left*\n",
        "- *We denote the output $\\overrightarrow{o_t}$ after reading the input $x_t$ from left to right*\n",
        "- *We denote the output $\\overleftarrow{o_t}$ after reading the input $x_t$ from right to left*\n",
        "\n",
        "The outputs returned by the RNN are stacked on top of each other (forward outputs and backward outputs).\n",
        "\n",
        "$$o_t = [\\overrightarrow{o_t}:\\overleftarrow{o_t}]$$\n",
        "\n",
        "\n",
        "The hidden states returned by the RNN are the final forward hidden state $\\overrightarrow{h_4^1}$ and the final backward hidden state $\\overleftarrow{h_0^1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pff-q_0boYJA",
        "colab_type": "text"
      },
      "source": [
        "### Relation\n",
        "\n",
        "Considering the time-major format, shape of the inputs and outputs of RNN are as follows:\n",
        "\n",
        "> Input shape: [max_len, batch_size]\n",
        "\n",
        ">Output shape: [max_len, batch_size, hidden_size * 2]\n",
        "\n",
        ">Hidden shape: [num_dir, batch_size, hidden_size]\n",
        "\n",
        "The Hidden shape is [2, batch_size, hidden_size]\n",
        "- hidden[0] is Final Forward Hidden state $\\overrightarrow{h_4^1}$\n",
        "- hidden[1] is Final Backward Hidden state $\\overleftarrow{h_0^1}$\n",
        "\n",
        "Outputs are stacked on top of each other (forward and backward) for each time step.\n",
        "The final time step's output contains the $$o_4^1 = [\\overrightarrow{o_4^1}:\\overleftarrow{o_4^1}]$$\n",
        "\n",
        "First set of hidden_dim states in the final time step's output is same as the final forward hidden state, i.e $$\\overrightarrow{o_4^1} == \\overrightarrow{h_4^1}$$\n",
        "\n",
        "Last set of hidden_dim states in the initial time step's output is same as the final backward hidden state, i.e $$\\overleftarrow{o_1^1} == \\overleftarrow{h_1^1}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5jACbwzj8rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Single_Layer_Bi_Directional_RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input shape => [max_len, batch_size]\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        # embed shape => [max_len, batch_size, embedding_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embed)\n",
        "        # output shape => [max_len, batch_size, hidden_size * 2] => since forward and backward outputs are stacked\n",
        "        # hidden shape => [2, batch_size, hidden_size]\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNzx_0tuoqX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e9419907-fbc8-467e-8801-99dff27eb840"
      },
      "source": [
        "n_layers = 1\n",
        "bidirectional = True\n",
        "model = Single_Layer_Bi_Directional_RNN(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, hidden = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 10])\n",
            "Hidden shape is : torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rhgZ5BEou9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :hidden_dim] == hidden[0]).all(), \"First hidden_dim of output at last time step must be same as Final Forward Hidden state in case of Single layer bi-directional RNN\"\n",
        "assert (output[0, :, hidden_dim:] == hidden[-1]).all(), \"Last hidden_dim of output at initial time step must be same as Final Backward Hidden state in case of Single layer bi-directional RNN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHJpkUj5pHpa",
        "colab_type": "text"
      },
      "source": [
        "## Multi Layer, Bi-Directional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5oZF32uo3PJ",
        "colab_type": "text"
      },
      "source": [
        "![mulit layer bi dir rnn](https://drive.google.com/uc?id=1vycOmrwoqUzqrOKj5WOaPePikciCjPxR)\n",
        "\n",
        "We will learn about 2 layer bi-directional RNN. The same concept can be applied to multiple layers.\n",
        "\n",
        "The input to the RNN in layer 1 is $X = \\{x_1, x_2, x_3, x_4\\}$. The Forward RNN of layer 1, reads input from left to right $$x_1 ... x_4$$ and Backward RNN, reads input from right to left $$x_4 ... x_1$$.\n",
        "\n",
        "The Forward and Backward RNN in the layer 1 outputs:\n",
        "- Forward outputs:  $$\\{\\overrightarrow{o_1^1}, \\overrightarrow{o_2^1}, \\overrightarrow{o_3^1}, \\overrightarrow{o_4^1}\\}$$\n",
        "- Backward outputs $$\\{\\overleftarrow{o_1^1}, \\overleftarrow{o_2^1}, \\overleftarrow{o_3^1}, \\overleftarrow{o_4^1}\\}$$\n",
        "\n",
        "\n",
        "Which are inputs to the Forward RNN and Backward RNN of layer 2.\n",
        "\n",
        "The outputs returned by the RNN are stacked on top of each other (forward outputs and backward outputs of layer 2). \n",
        "\n",
        "$$o_t = [\\overrightarrow{o_t^2}:\\overleftarrow{o_t^2}]$$\n",
        "\n",
        "\n",
        "The hidden states returned by the RNN are the final forward hidden state $$\\{\\overrightarrow{h_4^1}, \\overrightarrow{h_4^2}\\}$$ and the final backward hidden state $$\\{\\overleftarrow{h_0^1}, \\overleftarrow{h_0^2}\\}$$ of each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV8cSVaspidl",
        "colab_type": "text"
      },
      "source": [
        "### Relation\n",
        "\n",
        "Considering the time-major format, shape of the inputs and outputs of RNN are as follows:\n",
        "\n",
        "> Input shape: [max_len, batch_size]\n",
        "\n",
        "> Output shape: [max_len, batch_size, hidden_size * 2]\n",
        "\n",
        "> Hidden shape: [num_layers * num_dir, batch_size, hidden_size]\n",
        "\n",
        "The Hidden shape is [2 * 2, batch_size, hidden_size]\n",
        "\n",
        "Let's view Hidden states as *[num_layers, num_dir, batch_size, hidden_size]*\n",
        "- hidden[0][0] is Final Forward Hidden state of layer 1 : $\\overrightarrow{h_4^1}$\n",
        "- hidden[0][1] is Final Backward Hidden state of layer 1 : $\\overleftarrow{h_0^1}$\n",
        "- hidden[1][0] is Final Forward Hidden state of layer 2 : $\\overrightarrow{h_4^2}$\n",
        "- hidden[1][1] is Final Backward Hidden state of layer 2 : $\\overleftarrow{h_0^2}$\n",
        "\n",
        "Outputs are stacked on top of each other (forward and backward) for each time step of final layer.\n",
        "The final time step's output contains the $$o_4^2 = [\\overrightarrow{o_4^2}:\\overleftarrow{o_4^2}]$$\n",
        "\n",
        "First set of hidden_dim states in the final time step's output is same as the final forward hidden state of layer 2, i.e *$$\\overrightarrow{o_4^2} == \\overrightarrow{h_4^2}$$*\n",
        "\n",
        "Last set of hidden_dim states in the initial time step's output is same as the final backward hidden state of layer 2, i.e *$$\\overleftarrow{o_1^2} == \\overleftarrow{h_0^2}$$*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQKdAKgBqGoL",
        "colab_type": "text"
      },
      "source": [
        "### Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqEkCIhupb86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Multi_Layer_Bi_Directional_RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input shape => [max_len, batch_size]\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        # embed shape => [max_len, batch_size, embedding_dim]\n",
        "\n",
        "        output, hidden = self.rnn(embed)\n",
        "        # output shape => [max_len, batch_size, hidden_size * 2] => since forward and backward outputs are stacked\n",
        "        # hidden shape => [num_layers * 2, batch_size, hidden_size]\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fwhqXRCqLBb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "de862276-ae96-40e3-ab61-21ebc8328671"
      },
      "source": [
        "n_layers = 2\n",
        "bidirectional = True\n",
        "model = Multi_Layer_Bi_Directional_RNN(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, hidden = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 10])\n",
            "Hidden shape is : torch.Size([4, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMwJLQ4TqNYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "378d244b-a4a0-43e6-c78a-281184a58b7c"
      },
      "source": [
        "batch_size = sequence_tensor.shape[1]\n",
        "hidden = hidden.view(n_layers, 2, batch_size, hidden_dim)\n",
        "print(f\"Reshaped hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped hidden shape is : torch.Size([2, 2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqvQh3L6qQB3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :hidden_dim] == hidden[-1][0]).all(), \"First hidden_dim of output at last time step must be same as Final Forward Hidden state of final layer in case of Multi layer bi-directional RNN\"\n",
        "assert (output[0, :, hidden_dim:] == hidden[-1][1]).all(), \"Last hidden_dim of output at initial time step must be same as Final Backward Hidden state of final layer in case of Multi layer bi-directional RNN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIWBF8ImFrwl",
        "colab_type": "text"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJt1K6dIHKbF",
        "colab_type": "text"
      },
      "source": [
        "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
        "\n",
        "I highly recommend to go through the following resources for better understading of LSTM\n",
        "\n",
        "- [Colah blog on LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "- [Edwin Chen blog on exploring LSTMs](http://blog.echen.me/)\n",
        "- [Illustrated guide to LSTMs and GRUs](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "\n",
        "\n",
        "Summarizing LSTM equations:\n",
        "\n",
        "![lstm_eq](https://drive.google.com/uc?id=1RBRjWRkNWTLDDOvXfqwXcI1277InUYx7)\n",
        "\n",
        "\n",
        "Compared to RNN, the LSTM provides output, hidden and cell state instead of output and hidden state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNos4F9GUyV6",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwTenvm5FvHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input shape => [max_len, batch_size]\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        # embed shape => [max_len, batch_size, embedding_dim]\n",
        "\n",
        "        output, (hidden, cell) = self.lstm(embed)\n",
        "        # output shape => [max_len, batch_size, hidden_size]\n",
        "        # hidden shape => [num_layers * num_directions, batch_size, hidden_size]\n",
        "        # cell shape => [num_layers * num_directions, batch_size, hidden_size]\n",
        "\n",
        "        return output, (hidden, cell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQaVTmfeUs_X",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer, Uni-Directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnQjY0fQSRRx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fe79da84-08b8-4eb8-8cfd-25eb9087391b"
      },
      "source": [
        "n_layers = 1\n",
        "bidirectional = False\n",
        "model = LSTM(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden, cell) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")\n",
        "print(f\"Cell shape is : {cell.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 5])\n",
            "Hidden shape is : torch.Size([1, 3, 5])\n",
            "Cell shape is : torch.Size([1, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy38JiBIS5bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :] == hidden[0]).all(), \"Final output must be same as Hidden state in case of Single layer uni-directional LSTM\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYr-fcg8Unui",
        "colab_type": "text"
      },
      "source": [
        "## Multi Layer, Uni-Directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjrqqIlYTJmg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "daf8ffcb-ebac-4dc8-aa45-5758bd2f83de"
      },
      "source": [
        "n_layers = 2\n",
        "bidirectional = False\n",
        "model = LSTM(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden, cell) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")\n",
        "print(f\"Cell shape is : {cell.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 5])\n",
            "Hidden shape is : torch.Size([2, 3, 5])\n",
            "Cell shape is : torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRSGiSu6Tolo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :] == hidden[-1]).all(), \"Final output must be same as Final Hidden state in case of Multi layer uni-directional LSTM\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByxPGkK-Uh42",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer, Bi-Directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmK7vMdDT0KM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "458b0c9f-5077-4ca7-8688-9d65ddfbda16"
      },
      "source": [
        "n_layers = 1\n",
        "bidirectional = True\n",
        "model = LSTM(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden, cell) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")\n",
        "print(f\"Cell shape is : {cell.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 10])\n",
            "Hidden shape is : torch.Size([2, 3, 5])\n",
            "Cell shape is : torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICkcguLMT9Xs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :hidden_dim] == hidden[0]).all(), \"First hidden_dim of output at last time step must be same as Final Forward Hidden state in case of Single layer bi-directional LSTM\"\n",
        "assert (output[0, :, hidden_dim:] == hidden[-1]).all(), \"Last hidden_dim of output at initial time step must be same as Final Backward Hidden state in case of Single layer bi-directional LSTM\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP8PnAgDUXD6",
        "colab_type": "text"
      },
      "source": [
        "## Multi Layer, Bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qS4h4GUUD7r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6e58cf3c-87df-446e-b421-e7ba44754043"
      },
      "source": [
        "n_layers = 2\n",
        "bidirectional = True\n",
        "model = LSTM(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden, cell) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")\n",
        "print(f\"Cell shape is : {cell.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 10])\n",
            "Hidden shape is : torch.Size([4, 3, 5])\n",
            "Cell shape is : torch.Size([4, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqBWtt6fUGao",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77b96880-afa1-4ef1-9f02-992c06f0b5d9"
      },
      "source": [
        "batch_size = sequence_tensor.shape[1]\n",
        "hidden = hidden.view(n_layers, 2, batch_size, hidden_dim)\n",
        "print(f\"Reshaped hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped hidden shape is : torch.Size([2, 2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO3yOkvmUMaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :hidden_dim] == hidden[-1][0]).all(), \"First hidden_dim of output at last time step must be same as Final Forward Hidden state of final layer in case of Multi layer bi-directional LSTM\"\n",
        "assert (output[0, :, hidden_dim:] == hidden[-1][1]).all(), \"Last hidden_dim of output at initial time step must be same as Final Backward Hidden state of final layer in case of Multi layer bi-directional LSTM\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az3UKu0tVPO1",
        "colab_type": "text"
      },
      "source": [
        "# GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAWYTT3EVSnH",
        "colab_type": "text"
      },
      "source": [
        "GRU is also similar to LSTM. The internal gates implemented are different.\n",
        "\n",
        "![gru](https://drive.google.com/uc?id=1-2uNe1j6OzjoFf40HJImviggdhLLCURV)\n",
        "\n",
        "Refer to the following resources for GRU understanding:\n",
        "\n",
        "- https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRqMIL_xXAEw",
        "colab_type": "text"
      },
      "source": [
        "## GRU Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4hEa9CIWP-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, bidirectional):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # input shape => [max_len, batch_size]\n",
        "\n",
        "        embed = self.embedding(input)\n",
        "        # embed shape => [max_len, batch_size, embedding_dim]\n",
        "\n",
        "        output, hidden = self.gru(embed)\n",
        "        # output shape => [max_len, batch_size, hidden_size]\n",
        "        # hidden shape => [num_layers * num_directions, batch_size, hidden_size]\n",
        "\n",
        "        return output, (hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALdcN_NwXRb8",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer, Uni-Directional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17IxrS_VXQmA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "8d436cb4-9476-437f-a478-a33d9aecd3a4"
      },
      "source": [
        "n_layers = 1\n",
        "bidirectional = False\n",
        "model = GRU(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 5])\n",
            "Hidden shape is : torch.Size([1, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QOXjr9qXqFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :] == hidden[0]).all(), \"Final output must be same as Hidden state in case of Single layer uni-directional GRU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__3ZLmODYBE9",
        "colab_type": "text"
      },
      "source": [
        "## Multi Layer, Uni-Directional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VxeY0_MX9P1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a43b312c-64e9-487d-eac7-a7dafc55f5fc"
      },
      "source": [
        "n_layers = 2\n",
        "bidirectional = False\n",
        "model = GRU(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 5])\n",
            "Hidden shape is : torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K2CU840YE23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :] == hidden[-1]).all(), \"Final output must be same as Final Hidden state in case of Multi layer uni-directional GRU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKj4dNIxYRVT",
        "colab_type": "text"
      },
      "source": [
        "## Single Layer, Bi-Directional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhlG-mHkYLrk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bcf6f0c1-6181-445f-a76c-311aac440222"
      },
      "source": [
        "n_layers = 1\n",
        "bidirectional = True\n",
        "model = GRU(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 10])\n",
            "Hidden shape is : torch.Size([2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QObyw1P_YQQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :hidden_dim] == hidden[0]).all(), \"First hidden_dim of output at last time step must be same as Final Forward Hidden state in case of Single layer bi-directional GRU\"\n",
        "assert (output[0, :, hidden_dim:] == hidden[-1]).all(), \"Last hidden_dim of output at initial time step must be same as Final Backward Hidden state in case of Single layer bi-directional GRU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxCbgpxPY51w",
        "colab_type": "text"
      },
      "source": [
        "## Multi Layer, Bi-Directional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5toAWkExYW7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aff179a3-71cc-4db9-99af-4f6ecff794d0"
      },
      "source": [
        "n_layers = 2\n",
        "bidirectional = True\n",
        "model = GRU(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional)\n",
        "output, (hidden) = model(sequence_tensor)\n",
        "\n",
        "print(f\"Input shape is : {sequence_tensor.shape}\")\n",
        "print(f\"Output shape is : {output.shape}\")\n",
        "print(f\"Hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape is : torch.Size([8, 3])\n",
            "Output shape is : torch.Size([8, 3, 10])\n",
            "Hidden shape is : torch.Size([4, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltSiWExwYdkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee567810-eb7c-4953-9ba9-3424001cd913"
      },
      "source": [
        "batch_size = sequence_tensor.shape[1]\n",
        "hidden = hidden.view(n_layers, 2, batch_size, hidden_dim)\n",
        "print(f\"Reshaped hidden shape is : {hidden.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaped hidden shape is : torch.Size([2, 2, 3, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRfVqCPpYjKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert (output[-1, :, :hidden_dim] == hidden[-1][0]).all(), \"First hidden_dim of output at last time step must be same as Final Forward Hidden state of final layer in case of Multi layer bi-directional GRU\"\n",
        "assert (output[0, :, hidden_dim:] == hidden[-1][1]).all(), \"Last hidden_dim of output at initial time step must be same as Final Backward Hidden state of final layer in case of Multi layer bi-directional GRU\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9oLuctSYt89",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "![final](https://drive.google.com/uc?id=1WWXGWho5xrchcD7sIRIKeqKZDZrBnX6p)"
      ]
    }
  ]
}