{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Phase 1 - Machine Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrqYm_GHsYwJ",
        "colab_type": "text"
      },
      "source": [
        "# Phase 1: Basic Machine Translation with RNN\n",
        "\n",
        "In this phase we'll be building a machine learning model for translation task, using PyTorch and TorchText. Translation will be done from German to English using a specific type of architecture called Sequence to Sequence Models. The same architecture is being used for various tasks like summarization, utterance generation, etc.\n",
        "\n",
        "In this first phase, we'll start simple to understand the general concepts by implementing the model using RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spz2QVJvt72-",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The most common sequence-to-sequence (seq2seq) models are encoder-decoder models, which commonly use a recurrent neural network (RNN) to encode the source (input) sentence into a single vector. In this notebook, we'll refer to this single vector as a context vector. We can think of the context vector as being an abstract representation of the entire input sentence. This vector is then decoded by a second RNN which learns to output the target (output) sentence by generating it one word at a time.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1AZ8D6qSu_rMWOHa4STtLaPM4cKvTdZvL)\n",
        "\n",
        "The source sentence \"guten morgen\" is passed through an embedding layer and then input into the EncoderRNN (green). At each time step $t$, the input to the EncoderRNN is both the embedding of the current word $x_t$, $e(x_t)$ and the hidden state from the previous time step, $h_{t-1}$. Then the EncoderRNN outputs a new hidden state $h_t$. This can be represented as following:\n",
        "\n",
        "$$h_t = EncoderRNN(e(x_t), h_{t-1})$$\n",
        "\n",
        "\n",
        "We also append the `<sos>` and `<eos>` tokens to indicate the start and end of the sentence.\n",
        "\n",
        "Here, we will denote the source sentence as $X = \\{x_1, x_2, ...., x_T\\}$ where T is the number of words. We compute the hidden state for each time step as mentioned above, and we use the final hidden state $h_T$, as the context vector, i.e $h_T = z$. Intuitively the context vector represents the source sentence.\n",
        "\n",
        "Now we have the context vector, $z$, we can use it to start the decoding. The DecoderRNN (blue) is also similar to similar to EncoderRNN, at each time step, it takes the input as the embedding of the current word $y_t$, $e^{'}(y_t)$, as well as the hidden state  from the previous time step, $s_{t-1}$. Then the DecoderRNN outputs a new hidden state $s_t$. This can be represented as:\n",
        "\n",
        "$$s_t = DecoderRNN(e^{'}(y_t), s_{t-1})$$\n",
        "\n",
        "**Note**: The hidden state for the initial step of decoding is context vector, i.e $s_0 = z = h_T$, i.e the decoder initial hidden state is the final encoder hidden state.\n",
        "\n",
        "**Note**: The embedding layers are different for the Encoder and Decoder since the source and target languages are different. Though in diagram it is represented in the same color (yellow), we differentiate it in the equations as $e$ for source embedding and $e^{'}$ for target embedding layer.\n",
        "\n",
        "In the decoder, we need to go from the hidden state $s_t$ to an acutal word $\\hat{y}_t$. This will be done by passing the state $s_t$ through a linear layer which predicts the most probable word.\n",
        "\n",
        "$$\\hat{y}_t = softmax(f(s_t))$$\n",
        "\n",
        "The decoding happens step-by-step, i.e words are generated one after another. We always use `<sos>` as the first input to the decoder for the first time step, i.e $y_1 = $ \\<sos\\>. But for subsequent inputs, $y_{t>1}$ we use the actual, ground truth as the input.\n",
        "\n",
        "Since during the inference the ground truth is not available we will use the predicted word $\\hat{y}_t$ as the input word to the next time step $t+1$. During inference we will keep generating the words until the model outputs an \\<eos\\> token or for a certain number of steps.\n",
        "\n",
        "Once we have our predicted target sequence, $\\hat{Y} = \\{\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_T\\}$, we compare it against out actual target sentence, $Y = \\{y_1, y_2, ..., y_T\\}$, to calculate our loss. We then use this loss to update the parameters of the model.\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx11nfWEuRZ4",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1tbwR6oqnOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import math\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zVto0-Ave7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download de"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CYRvHG2vjhK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrBwEL_xSRjd",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nop_dpsBvoaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the German text from a string into a list of tokens\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # Implement the tokenization of german sentence using spacy_de\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ddeb5yIPwKev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes the English text from a string into a list of tokens\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    # Implement the tokenization of english sentence using spacy_en\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQTdNTaUAjFR",
        "colab_type": "text"
      },
      "source": [
        "### Fields\n",
        "\n",
        "Learn about Torchtext Fields here:\n",
        "\n",
        "- [Youtube Tutorials](https://www.youtube.com/watch?v=KRgq4VnCr7I)\n",
        "- [Blog](http://anie.me/On-Torchtext/)\n",
        "\n",
        "In Machine Translation, the source language and target languages are different.\n",
        "\n",
        "Field defines how the data should be processed.\n",
        "\n",
        "Since we are tokenizing using spacy, we can pass our tokenizer method to the argument tokenizer\n",
        "\n",
        "In order to indicate the starting and ending of a sentence, we can init_token as <sos> and eos_token as <eos>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dqkj9MguAiUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SRC = Field(\n",
        "        tokenize=tokenize_de,\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        lower=True)\n",
        "\n",
        "TRG = Field(\n",
        "        tokenize=tokenize_en,\n",
        "        init_token='<sos>',\n",
        "        eos_token='<eos>',\n",
        "        lower=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJB9PMrMAnKS",
        "colab_type": "text"
      },
      "source": [
        "### Translation Dataset\n",
        "\n",
        "We will be using Multi30K dataset. Torchtext provides support for multi open datasets. This is a dataset with ~31,000 parallel English, German and French sentences.\n",
        "\n",
        "exts specifies which languages to use as the source and target (source goes first) and fields specifies which field to use for the source and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFoEyNZHAmhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdRF_xIzAuZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTFxIwnGAvuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vnxYp2UAy-v",
        "colab_type": "text"
      },
      "source": [
        "### Vocabulary\n",
        "\n",
        "Using the **min_freq** argument, we only allow tokens that appear at least 2 times to appear in our vocabulary. Tokens that appear only once are converted into an **\\<unk\\>** (unknown) token.\n",
        "\n",
        "It is important to note that our vocabulary should only be built from the training set and not the validation/test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loJDackbAxrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "# build vocabulary on SRC, TRG fields using training data and a min_freq of 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ap8PWcBLZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
        "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOZr20rYBOLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsElxop-CcCL",
        "colab_type": "text"
      },
      "source": [
        "### Iterators\n",
        "\n",
        "The final step of preparing the data is to create the iterators. These can be iterated on to return a batch of data which will have a src attribute (the PyTorch tensors containing a batch of numericalized source sentences) and a trg attribute (the PyTorch tensors containing a batch of numericalized target sentences).\n",
        "\n",
        "When we get a batch of examples using an iterator we need to make sure that all of the source sentences are padded to the same length, the same with the target sentences. Luckily, TorchText iterators handle this for us!\n",
        "\n",
        "We use a BucketIterator instead of the standard Iterator as it creates batches in such a way that it minimizes the amount of padding in both the source and target sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S8XmUQMB6Ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DGvTSSVCiiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xPWMaZwCrOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample checking\n",
        "temp = next(iter(train_iterator))\n",
        "temp.src.shape, temp.trg.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsa1to8OzP1v",
        "colab_type": "text"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The encoder we are using is a 2 layer RNN. The paper we are implementing uses a 4-layer LSTM, but in the interest of training time we cut this down to 2-layers.\n",
        "\n",
        "Once the source sentence is passed through the RNN, the final step's hidden state in all the layers will be used for decoding.\n",
        "\n",
        "This final hidden state is called context vector, which represents the encoding of the source sentence. This will be used as the initial hidden state for decoder.\n",
        "\n",
        "> Note: In order for this to work, the number of layers in encoder and decoder must be same, if not then some extra strategy needs to be applied. For example, if the encoder is 1 layer and decoder is 2 layers means, then encoder final hidden state needs to replicated and sent as initial state to decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ed8VVWwv_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # TODO\n",
        "        # Declare the embedding layer\n",
        "        # self.embedding = \n",
        "\n",
        "        # TODO\n",
        "        # Declare the encoder rnn\n",
        "        # self.rnn = \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, src):\n",
        "        # TODO\n",
        "        # comment the src shape\n",
        "\n",
        "        embedded_input = self.embedding(src)\n",
        "        embedded_input = self.dropout(embedded_input)\n",
        "        # TODO\n",
        "        # comment the embedded_input shape\n",
        "\n",
        "        outputs, (hidden) = self.rnn(embedded_input)\n",
        "        # TODO\n",
        "        # comment the shapes of outputs and hidden\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W31gOnUb5tXF",
        "colab_type": "text"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder we are using is a 2 layer RNN like encoder. The Decoder class does a single step of decoding, i.e. it ouputs single token per time-step.\n",
        "\n",
        "The initial hidden and cell states to the decoder are context vectors $z^1, z^2$, which are the final hidden and cell states of the encoder from the same layer.\n",
        "\n",
        "As we are only decoding one token at a time, the input tokens will always have a sequence length of 1. We unsqueeze the input tokens to add a sentence length dimension of 1. Then it is passed through the RNN. There is an additional Linear layer, used to make the predictions from the top layer hidden state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ml0sEEc0s6G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # TODO\n",
        "        # declare the embedding layer\n",
        "        # self.embedding = \n",
        "\n",
        "        # TODO\n",
        "        # declare the decoder rnn\n",
        "        # self.rnn = \n",
        "        \n",
        "        # TODO\n",
        "        # declare the output prediction layer\n",
        "        # self.out = \n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, dec_input, hidden):\n",
        "        # TODO\n",
        "        # comment the shapes of dec_input & hidden inputs\n",
        "\n",
        "        dec_input = dec_input.unsqueeze(0)\n",
        "        # TODO\n",
        "        # comment why this is required\n",
        "\n",
        "        dec_input_embedded = self.embedding(dec_input)\n",
        "        dec_input_embedded = self.dropout(dec_input_embedded)\n",
        "        # TODO\n",
        "        # comment the shape of dec_input_embedded\n",
        "\n",
        "        output, (hidden) = self.rnn(dec_input_embedded, (hidden))\n",
        "        # TODO\n",
        "        # comment the shapes of output, hidden\n",
        "\n",
        "        # TODO\n",
        "        # pass the output through the prediction layers and get the logits of tokens\n",
        "        # prediction = \n",
        "\n",
        "        return prediction, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X5IXBKW5vpr",
        "colab_type": "text"
      },
      "source": [
        "### Sequence-to-Sequence Model\n",
        "\n",
        "\n",
        "For the final part of the implemenetation, we'll implement the seq2seq model. This will handle:\n",
        "\n",
        "- receiving the input/source sentence\n",
        "- using the encoder to produce the context vectors\n",
        "- using the decoder to produce the predicted output/target sentence\n",
        "\n",
        "The forward method takes the source sentence, target sentence.\n",
        "\n",
        "When decoding, at each time-step we will predict what the next token in the target sequence will be from the previous tokens decoded, $\\hat{y}_{t+1}=f(s_t^L)$. We will use the actual ground-truth next token in the sequence as the input to the decoder during the next time-step.\n",
        "\n",
        "The first input to the decoder is the start of sequence <sos> token. As our trg tensor already has the <sos> token appended we get our $y_1$ by slicing into it. We know how long our target sentences should be (max_len), so we loop that many times. The last token input into the decoder is the one before the <eos> token\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jZRTqnm4Ghw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        # TODO\n",
        "        # comment the shapes of src and trg\n",
        "\n",
        "        # TODO\n",
        "        # get the values of the following from inputs\n",
        "        # batch_size = \n",
        "        # trg_len = \n",
        "        # trg_vocab_size = \n",
        "\n",
        "        # outputs => to store the predictions at each decoding step\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
        "\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        # TODO\n",
        "        # initial input to decoder is <sos> which indicates to start the decoding process\n",
        "        # since the first token is always <sos> take that as initial decoder input\n",
        "        # decoder_input = \n",
        "\n",
        "        # Now let's run the decoding till the trg_len steps\n",
        "        # skipping the first step as we have taken input as <sos>\n",
        "        for step in range(1, trg_len):\n",
        "            # decode step\n",
        "            output, hidden = self.decoder(decoder_input, hidden)\n",
        "\n",
        "            # save the output\n",
        "            outputs[step] = output\n",
        "\n",
        "            # TODO\n",
        "            # get the input for decoder, for the next step\n",
        "            # decoder_input = \n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syb2F9K_2rIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = \n",
        "OUTPUT_DIM = \n",
        "ENC_EMB_DIM = \n",
        "DEC_EMB_DIM = \n",
        "HID_DIM = \n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJXcEZTs21Q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRzpinv13GPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w82dWRu63VRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxB6MzAN_88U",
        "colab_type": "text"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hu0dGfa3j6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1E0XliPX__Si",
        "colab_type": "text"
      },
      "source": [
        "### Loss Criterion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49WQhvSh3npw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "# get the padding index from the target vocabulary\n",
        "# TRG_PAD_IDX = \n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW4eAhRY_26o",
        "colab_type": "text"
      },
      "source": [
        "### Training Method\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "- get the source and target sentences from the batch, $X$ and $Y$\n",
        "- zero the gradients calculated from the last batch\n",
        "- feed the source and target into the model to get the output, $\\hat{Y}$\n",
        "- as the loss function only works on 2d inputs with 1d targets we need to flatten each of them with .view\n",
        "- we slice off the first column of the output and target tensors (<sos> not used)\n",
        "- calculate the gradients with loss.backward()\n",
        "- clip the gradients to prevent them from exploding\n",
        "- update the parameters of our model by doing an optimizer step\n",
        "- update the loss\n",
        "\n",
        "Finally, we return the loss that is averaged over all batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUnT6LJ531y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        # TODO\n",
        "        # comment the shapes of the src, trg\n",
        "\n",
        "        # zero the gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # forward pass\n",
        "        output = model(src, trg)        \n",
        "\n",
        "        # TODO\n",
        "        # reshape the output, trg to make it compatible for loss cal.\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient clipping\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # update the parameters of the model\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the loss\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkwPU7Hj_0J-",
        "colab_type": "text"
      },
      "source": [
        "### Evaluation Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coV81YI2-RrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "            # TODO\n",
        "            # comment the shapes of the src, trg\n",
        "\n",
        "            # forward pass\n",
        "            output = model(src, trg)        \n",
        "\n",
        "            # TODO\n",
        "            # reshape the output, trg to make it compatible for loss cal.\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            # update the loss\n",
        "            epoch_loss += loss.item()\n",
        "    \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty1BD942-2aJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = elapsed_time - (elapsed_mins * 60)\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICC92GsX_wrQ",
        "colab_type": "text"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC-XGB4K-5Yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 2\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, criterion, optimizer, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # TODO\n",
        "    # update the best validation loss and save the model \n",
        "    \n",
        "    print(f\"Epoch {epoch + 1} | Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} |\")\n",
        "    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f} |\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfiZzbi6EmTZ",
        "colab_type": "text"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1WlYgT9EnhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "# load the trained model\n",
        "\n",
        "# TODO\n",
        "# calculate test loss by using test iterator\n",
        "test_loss = \n",
        "\n",
        "print(f\"\\tTest Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}