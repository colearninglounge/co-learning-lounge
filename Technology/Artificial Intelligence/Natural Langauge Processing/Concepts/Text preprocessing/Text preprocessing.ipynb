{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing\n",
    "\n",
    "Text can come in a variety of forms from a list of individual words, to sentences to multiple paragraphs with special characters (like tweets for example). This step is very important in pipeline which helps to feed right data to the model.\n",
    "**Text preprocessing seems severely overlooked topic.** A few people I spoke to mentioned inconsistent results from their NLP applications only to realize that they were not  preprocessing their text or were using the wrong kind of text preprocessing for their project.\n",
    "<img src=\"../../Images/Text preprocessing.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "-  Introduction\n",
    "-  Dataset\n",
    "-  Types of text preprocessing techniques\n",
    "   - Lowercasing\n",
    "   - Tokenization\n",
    "     - Sentence tokenization\n",
    "     - Word tokenization\n",
    "   - Normalization\n",
    "      - Lemmatization\n",
    "      - Stemming\n",
    "   - Cleaning\n",
    "     - Noise removal\n",
    "       - Removal of HTML characters, URL, Expressions like [laughing], [Crying], Alphanumeric character\n",
    "     - Punctuation\n",
    "     - Stopwords\n",
    "   - How is lemmatizing different from stemming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, you will discover how you can clean and prepare your text ready for modeling with machine learning model. <br>\n",
    "There is a whole suite of text preparation methods that you may need to use, and the choice of methods really depends on your natural language processing task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's consider below opinion related to iPhone extracted tweet from Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"RT @Joydeep_911: I ‚ô• my &lt;3 iphone &amp; you‚Äôr√® awsm apple. DisplayIsAwesome ‚òÖ‚òÖ‚òÖ, sooo happppppy üôÇ http://www.apple.com. I have plans to buy 2 more of same varient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of text preprocessing techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "This is the basic and must have preprocessing step in the pipeline. Text often has a variety of capitalization reflecting the beginning of sentences, proper nouns emphasis. Lowercasing solves the sparsity issue, where the same words with different cases map to the same lowercase form: 'Canada' vs. 'canada'\n",
    "> But it is important to remember that some words, like ‚ÄúUS‚Äù to ‚Äúus‚Äù, can change meanings when reduced to the lower case.\n",
    "\n",
    "Python has rich set of __[string functions](https://www.w3schools.com/python/python_ref_string.asp)__. Will use lower() function to convert string into lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @joydeep_911: i ‚ô• my &lt;3 iphone &amp; you‚Äôr√® awsm apple. displayisawesome ‚òÖ‚òÖ‚òÖ, sooo happppppy üôÇ http://www.apple.com. i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lowercasing\n",
    "\n",
    "lowercased_text = text.lower()\n",
    "lowercased_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Text normalization is the process of transforming text into a canonical (standard) form. For example, the word ‚Äúloooove‚Äù and ‚Äúluv‚Äù can be transformed to ‚Äúlove‚Äù, its canonical form. Another example is mapping of near identical words such as ‚Äústopwords‚Äù, ‚Äústop-words‚Äù and ‚Äústop words‚Äù to just ‚Äústopwords‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unicode normalization is one of the techinque where special character(√§, √∂, √º, √ü) has to be normalized. <br>\n",
    "It is necessary to keep the complete data in standard encoding format. UTF-8 encoding is widely accepted and is recommended to use. Read __[this](https://stackoverflow.com/questions/2241348/what-is-unicode-utf-8-utf-16)__ stack overflow answer to know details about Unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @joydeep_911: i  my &lt;3 iphone &amp; youre awsm apple. displayisawesome , sooo happppppy  http://www.apple.com. i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In the next step √® from you‚Äôr√® normalized to e and other non-ascii character like ‚ô• and ‚òÖ is ignored.\n",
    "\n",
    "from unicodedata import normalize\n",
    "unicode_norm_text = normalize('NFD', lowercased_text).encode('ascii', 'ignore')\n",
    "unicode_norm_text = unicode_norm_text.decode('UTF-8')\n",
    "unicode_norm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions are words that we write with an apostrophe: \"ain‚Äôt\" or \"aren‚Äôt\". Since we want to normalize the text, it makes sense to expand these contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @joydeep_911: i  my &lt;3 iphone &amp; you are awsm apple. displayisawesome , sooo happppppy  http://www.apple.com. i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## contractions.json from util folder has whole list of contraction.\n",
    "\n",
    "\"\"\"\n",
    "{\n",
    "  \"ain't\": \"am not\",\n",
    "  \"aren't\": \"are not\",\n",
    "  \"can't\": \"cannot\"\n",
    "  ...\n",
    "  }\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "\n",
    "## Contraction dictionary is stored in JSON file as Python dictionary\n",
    "contractions_data=open(\"../../utils/contractions.json\", encoding=\"utf8\").read()\n",
    "contractions = json.loads(contractions_data)\n",
    "\n",
    "## Expanding contraction\n",
    "for key, value in contractions.items() :                  \n",
    "    if key in unicode_norm_text :\n",
    "        contraction_norm_text = re.sub(r'\\b'+key+r'\\b',value,unicode_norm_text)\n",
    "contraction_norm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.<br>\n",
    "Misspellings can be easily corrected with paid spell checking API services like Google and __[Azure spell checker](https://azure.microsoft.com/en-in/services/cognitive-services/spell-check/)__.\n",
    "<img src=\"Azure_spell_check_output.PNG\" alt=\"Drawing\" style=\"width: 1050px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @joydeep_911: i  my &lt;3 iphone &amp; you are awsm apple. display is awesome , sooo happppppy  http://www.apple.com. i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Here manually spelling is corrected.\n",
    "\n",
    "spell_corrected_text = re.sub(\"displayisawesome\", \"display is awesome\", contraction_norm_text)\n",
    "spell_corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are scenario's where even neural network based spell checker module couldn't correct like **awsm** and **sooo happppppy**. For large dataset it could be painful task and might not be scalable. Here problem can be solved using regex after indentifying pattern(repeated letter for more than 2 times) but it crudely harming **www**. So very naive regex expression might mess up original text and it might be difficult to come with global regex which might solve all miss spell issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @joydeep_911: i  my &lt;3 iphone &amp; you are awsm apple. display is awesome , soo happy  http://ww.apple.com. i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "norm_text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(spell_corrected_text))\n",
    "norm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "### Noise removal\n",
    "Noise removal is about removing characters, digits and pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps and one of the first things you should be looking into when it comes to Text Mining and NLP. <br>\n",
    "There are various ways to remove noise. This includes punctuation removal, special character removal, numbers removal, html formatting removal, domain specific keyword removal (e.g. ‚ÄòRT‚Äô for retweet), source code removal, header removal and more. It all depends on which domain you are working in and what entails noise for your task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data obtained from web usually contains a lot of html entities like &lt; &gt; &amp; which gets embedded in the original data. \n",
    "It is thus necessary to get rid of these entities. One approach is to directly remove them by the use of specific regular expressions. \n",
    "Another approach is to use appropriate packages and modules (for example **htmlparser** or **BeautifulSoup** of Python), which can convert these entities to standard html tags. \n",
    "For example: \"&lt\"; is converted to ‚Äú<‚Äù and \"&amp\"; is converted to ‚Äú&‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @joydeep_911: i  my <3 iphone & you are awsm apple. display is awesome , soo happy  http://ww.apple.com. i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cleaning\n",
    "\n",
    "## HTML tags\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(norm_text)\n",
    "\n",
    "clean_text = soup.get_text()\n",
    "\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i  my <3 iphone & you are awsm apple. display is awesome , soo happy  . i have plans to buy 2 more of same varient'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cleaning\n",
    "\n",
    "## URL etc\n",
    "\n",
    "clean_text = re.sub(\"http.*\\.com\",\"\",clean_text)\n",
    "clean_text = re.sub(\"rt\\s+@.*:\",\"\",clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### Sentence tokenization\n",
    "Sentence tokenization is dividing a string of written language into its component sentences. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' i  my <3 iphone & you are awsm apple.',\n",
       " 'display is awesome , soo happy  .',\n",
       " 'i have plans to buy 2 more of same varient']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "tokenized_sentences = sent_tokenize(clean_text)\n",
    "\n",
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization\n",
    "Word tokenization is dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.\n",
    "\n",
    "Most of NLP python library has function to perform sentence and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'my', '<', '3', 'iphone', '&', 'you', 'are', 'awsm', 'apple', '.'],\n",
       " ['display', 'is', 'awesome', ',', 'soo', 'happy', '.'],\n",
       " ['i', 'have', 'plans', 'to', 'buy', '2', 'more', 'of', 'same', 'varient']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization\n",
    "\n",
    "tokenized_words = []\n",
    "# Word tokenization\n",
    "for sent in tokenized_sentences :\n",
    "    tokenized_words.append(word_tokenize(sent))\n",
    "\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "All the punctuation marks according to the priorities should be dealt with. For example: ‚Äú.‚Äù, ‚Äú,‚Äù,‚Äù?‚Äù are important punctuations that should be retained while others need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'my', '', '3', 'iphone', '', 'you', 'are', 'awsm', 'apple', ''],\n",
       " ['display', 'is', 'awesome', '', 'soo', 'happy', ''],\n",
       " ['i', 'have', 'plans', 'to', 'buy', '2', 'more', 'of', 'same', 'varient']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation_cleaned_text = []\n",
    "import string\n",
    "#table = str.maketrans('', '', re.sub(\"!\\.\\?\",'',string.punctuation)) ## What is this maketrans table?\n",
    "\n",
    "## https://www.programiz.com/python-programming/methods/string/maketrans\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for sent in tokenized_words :\n",
    "    punctuation_cleaned_text.append([word.translate(table) for word in sent])\n",
    "punctuation_cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "When applying machine learning to text, these words can add a lot of noise hence it needs to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', '3', 'iphone', '', 'awsm', 'apple', ''],\n",
       " ['display', 'awesome', '', 'soo', 'happy', ''],\n",
       " ['plans', 'buy', '2', 'varient']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_cleaned_text = []\n",
    "import nltk\n",
    "stop = nltk.corpus.stopwords.words('english')\n",
    "for sent in punctuation_cleaned_text : \n",
    "    stopwords_cleaned_text.append([i for i in sent if i not in stop])\n",
    "\n",
    "stopwords_cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing\n",
    "It's the process¬†of grouping together the inflected forms of a word¬†so they can be analyzed as a single term,¬†identified by the word's lemma.¬†The lemma is the canonical form of a set of words.¬†More simply put, lemmatizing is using vocabulary analysis¬†of words to remove inflectional endings¬†and return to the dictionary form of a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 3 iphone  awsm apple ', 'display awesome  soo happy ', 'plan buy 2 varient']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_text = []\n",
    "for stopwords_cleaned_sent in stopwords_cleaned_text :\n",
    "    lemmatized_text.append(\" \".join([lemma.lemmatize(word) for word in stopwords_cleaned_sent]))\n",
    "\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "It is¬†the process of reducing inflected or derived words¬†to their word stem or root.\n",
    "More simply put, the process of stemming means¬†often crudely chopping off the end of a word,¬†to leave only the base.¬†So this means taking words with various suffixes¬†and condensing them under the same root word.\n",
    "\n",
    "A few that are included in the NLTK package are¬†the Porter Stemmer, the Snowball Stemmer,¬†the Lancaster Stemmer, and a Regex-Based Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 3 iphon  awsm appl ', 'display awesom  soo happi ', 'plan buy 2 varient']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem(\"plans\")\n",
    "\n",
    "stemmed_text = []\n",
    "for stopwords_cleaned_sent in stopwords_cleaned_text :\n",
    "    stemmed_text.append(\" \".join([ps.stem(word) for word in stopwords_cleaned_sent]))\n",
    "\n",
    "stemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How is lemmatizing different from stemming? <br>\n",
    "The goal of both is to condense derived words into their base forms.\n",
    "<img src=\"../../Images/Stemming_Lemmatization.PNG\" style=\"width: 550px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stemmed</td>\n",
       "      <td>stem</td>\n",
       "      <td>Stemmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stemming</td>\n",
       "      <td>stem</td>\n",
       "      <td>Stemming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Electricity</td>\n",
       "      <td>electr</td>\n",
       "      <td>Electricity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Electrical</td>\n",
       "      <td>electr</td>\n",
       "      <td>Electrical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Berries</td>\n",
       "      <td>berri</td>\n",
       "      <td>Berries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Berry</td>\n",
       "      <td>berri</td>\n",
       "      <td>Berry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words stemming    lemmatize\n",
       "0      Stemmed     stem      Stemmed\n",
       "1     Stemming     stem     Stemming\n",
       "2  Electricity   electr  Electricity\n",
       "3   Electrical   electr   Electrical\n",
       "4      Berries    berri      Berries\n",
       "5        Berry    berri        Berry"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## In this exercise stemmer is giving wrong result for Electricity, Electrical, Berries, Berry as it is crudely chopping off\n",
    "## Where as Lemmatization output is better.\n",
    "\n",
    "import pandas as pd\n",
    "words = [\"Stemmed\", \"Stemming\", \"Electricity\", \"Electrical\", \"Berries\", \"Berry\"]\n",
    "\n",
    "lemma_stemming_df = pd.DataFrame({\"words\" : words},columns=['words', 'stemming','lemmatize'])\n",
    "\n",
    "for word in words: \n",
    "    lemma_stemming_df.loc[lemma_stemming_df[\"words\"] == word, \"stemming\"] = ps.stem(word)\n",
    "    lemma_stemming_df.loc[lemma_stemming_df[\"words\"] == word, \"lemmatize\"] = lemma.lemmatize(word)\n",
    "lemma_stemming_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This is extensive list of steps to clean the messy text. Not all steps are applicable to all text related problem so choose judiciously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This tutorial is intended to be a public resource. As such, if you see any glaring inaccuracies or if a critical topic is missing, please feel free to point it out or (preferably) submit a pull request to improve the tutorial. Also, we are always looking to improve the scope of this article. For anything feel free to mail us @ yogesh.kothiya.101@gmail.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Author of this article is Yogesh Kothiya. You can follow him on __[LinkedIn](https://www.linkedin.com/in/yogeshkothiya/)__, __[Medium](https://medium.com/@kothiya.yogesh)__, __[GitHub](https://github.com/kothiyayogesh)__, __[Twitter](https://twitter.com/Yogesh_Kothiya)__.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
