{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dogs_vs_cats.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfTK5BgKypzA",
        "colab_type": "text"
      },
      "source": [
        "# DOGS VS CATS  CLASSIFIER\n",
        "\n",
        "#### Have you ever wondered how we can classify cats and dogs using deep learning with the help of transfer learning. If yes, then you are about to find out how and implement the solution by yourselves. It is primarily a image classification problem.<br><br>\n",
        "\n",
        "\n",
        "![Dogs vs Cats Classifier](https://drive.google.com/uc?id=1grUxufxaXpp27S0hXf4NkGbg3rge4kei)\n",
        "\n",
        "\n",
        "<br><br>Post your doubt/feedback/discussion in our FB group unit [here](https://www.facebook.com/groups/colearninglounge/) in the appropriate section.\n",
        "\n",
        "## Table of content:\n",
        "\n",
        "*   Introduction\n",
        "*   Problem Statement\n",
        "*   Installing Dependencies\n",
        "*   Importing necessary packages\n",
        "*   Preprocessing Dataset\n",
        "*   Create Model/Network\n",
        "*   Train the model\n",
        "*   Prediction Time\n",
        "*   Save Model\n",
        "*   Using Pretrained Model\n",
        "*   Running Our Digit Recognizer\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this tutorial, we'll implement  dogs vs cats classifier using keras. We will cover how to implement a image classification problem using transfer learning. We will first cover some basics of Image classification like reading in, processing data and encoding the labels.Then we'll cover how to use network like VGG16  using keras and then we will introduce how to train and evaluate a model. We will also cover some intermediate level  topics like saving a model and reusing a saved model.\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "We need to predict the labels for the images of dogs and cats using deep learning with the help of transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29995MpUvDwR",
        "colab_type": "text"
      },
      "source": [
        "## Importing necessary packages\n",
        "As in every python/any language we need to import the necessary libraries.The below modules are necessary for our demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m43s7Q_qsvVE",
        "colab_type": "code",
        "outputId": "6c1e8baa-bed7-43b0-ced9-33b0d760174c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import layers\n",
        "from keras import preprocessing\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications import VGG16\n",
        "from keras.models import  Model,model_from_json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(1337)\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgYysGWpv3PM",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing Dataset\n",
        "Any machine learning or deep learning program start with dataset preparation. Dataset preparation involves cleansing the data, labeling the data, splitting the dataset into train and test  etc. Fortunately  we dont need to prepare the dataset since we are using open source dataset  [KAGGLE DOGS VS CATS](https://www.kaggle.com/c/dogs-vs-cats/data) dataset which contains images of dogs and cats and these are also labelled for us to use. Train dataset has 25000 images and test dataset has 12500 images. First we need to download the dataset from kaggle and arrange it in the following manner.<br><br>\n",
        "\n",
        "![Dataset Splitting](https://drive.google.com/uc?id=1oe_u726UIZ7u1AIgg8Eyz_xntbd9Jz2A)\n",
        "\n",
        "<br>We first unzip train.zip and test1.zip files using \"gzip train.zip\" and \"gzip test1.zip\". We then create a folder in test1 as test and move all the files to test. You may be thinking why we need to do this? Its because \"flow_from_directory\" keras module expects the train and test dataset should be in that format only. First we need to load the images in batches using keras \"ImageDataGenerator\".  Here \"preprocess_input\" is used for loading images in batches. \"flow_from_directory\" method is used to load the images and apply transfomations to it like reshapping the image to (224,224,3) using color_mode='rgb' and class_mode is used to create labels for the images automatically. So, In our case we have two classes so labels of 0(cats) and 1(dogs) 's will be loaded according to their images. We are also shuffling the images for better quality of training dataset.\n",
        "\n",
        "Note: Mention the paths as per your dataset location. Mentioned train and test dataset path are only for demo purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntXPc4YVtcpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_split_data(self):\n",
        "    train_datagen = preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "    self.train_generator = train_datagen.flow_from_directory(directory=\"/home/dgambali/repos_last/deep_learning/kaggle_dog_cats/train\",\n",
        "        target_size=(224, 224),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=25,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=True,\n",
        "        seed=42\n",
        "    )\n",
        "    test_datagen = preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "    self.test_generator = test_datagen.flow_from_directory(\n",
        "        directory=\"/home/dgambali/repos_last/deep_learning/kaggle_dog_cats/test1\",\n",
        "        target_size=(224, 224),\n",
        "        color_mode=\"rgb\",\n",
        "        batch_size=25,\n",
        "        class_mode=None,\n",
        "        shuffle=False,\n",
        "        seed=42\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY8MsJsCwROV",
        "colab_type": "text"
      },
      "source": [
        "## Use Pretrained Model using Transfer Learning\n",
        "The next step in Deep learning is to decide on the network/model/deep learning algorthm to use. Since we are dealing with classification problem we are using [ VGG16](http://yann.lecun.com/exdb/lenet/) architecture. You can use any image classification networks like Googlenet, Resnet, Squeezenet and experiment. So, we need to use VGG16 algorithm with imagenet weights and without the last layer i.e Classification layer, \"include_top\" will remove the last layer, for this we are using keras.applications.VGG16 to do that. The output of VGG16 is 1000 class labels but we don't want all 1000 for our demo we need only 2 class labels. So, in order to do that we need to modify the last layer i.e FullyConnected aka Dense layer output to 2 instead of 1000. We did exactly that using the Dense layer in keras. After that we need to construct a model with Feature learning from VGG16 and classifier from newly created model. So, we are calling keras.models.Model to do exactly that. The final step is to train our classifier, inorder to do that we make all 20 feature learning layers trainable parameter to False. We only wanted to train our last layer so we set layers after 20 layers trainable parameter to True.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCXgz6XFtgNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_model_using_transfer_learning(self):\n",
        "    base_model = VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=False)\n",
        "    x = base_model.output\n",
        "    x = layers.Flatten()(x)\n",
        "    # x=layers.Dense(4096, activation='relu')(x)\n",
        "    x = layers.Dense(self.no_of_classes, activation='softmax')(x)\n",
        "    self.model = Model(inputs=base_model.input, outputs=x)\n",
        "    for layer in self.model.layers[:20]:\n",
        "        layer.trainable = False\n",
        "    for layer in self.model.layers[20:]:\n",
        "        layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0lTcD8xwXR7",
        "colab_type": "text"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "So, its time for training our model. For training a model we need two things. One loss function and other optimizer. Loss function will try to calculate difference between observed and actual. Optimizer will try to optimize the loss function in a way it learns the weights faster. So, for this demo we choose \"binary_crossentropy\" as loss function and optimizer as \"adam\". We are using \"binary_crossentropy\" because we need to classify two objects if you want to classify more than two objects you can use \"categorical_crossentropy\". You can experiment with loss and optimizer to check which fits best for your algorithm. First we need to attach the loss and optimizer to our model and then using fit function we train our model using keras \"fit_generator\" function which we use to train our model by passing the train dataset. Here you can see two more arguments \"epoch\" and \"batch_size\". Here epoch means how many times you want to repeat your train data. For each epoch our model learns more and more about our data. You may under the assumption that if more epoch = more accurary thats true partially because there will be a saturation point after a certain epoch your model accurary doesn't change. So, epoch should be choosen wisely. Batchsize is how many input dataset items to train parallely. So, the advantage your computational power decreases and model can learn better parallely. Batch size has a limitation on the computational power. If you put more batchsize your machine may say out of memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS3EUstTtj0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(self):\n",
        "    STEP_SIZE_TRAIN = self.train_generator.n // self.train_generator.batch_size\n",
        "    self.model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    self.model.fit_generator(generator=self.train_generator,\n",
        "                        steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                        epochs=self.no_of_epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnjD3VvWX-In",
        "colab_type": "text"
      },
      "source": [
        "## Save Model\n",
        "\n",
        "We don't need to train our model everytime we wanted to test or make predictions. We can save our model once we train our model. So, we store our model in json format. We need to save our weights as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-lKvWsKtoXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(self):\n",
        "    model_json = self.model.to_json()\n",
        "    with open(\"%s.json\" % self.save_model_name, \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    self.model.save_weights(\"%s.h5\" % self.save_model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw16fjjawl_T",
        "colab_type": "text"
      },
      "source": [
        "## Prediction Time\n",
        "\n",
        "So its time for inference. predict_generator will do the inference on the test dataset  So, keras model has a function predict_generator which will predict based on the training. We pass test dataset of 10000 data points and a batch size. The output of the function is a 12500*2tensor, we use argmax(axis=1) function to get max index value of 1x2 tensor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrjQNexltpH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_output(self):\n",
        "    test_steps = self.test_generator.n // self.test_generator.batch_size\n",
        "    predict_model = self.model.predict_generator(generator=self.test_generator, steps=test_steps)\n",
        "    id = np.array([i + 1 for i in range(len(predict_model))])\n",
        "    df = pd.DataFrame({\"id\": id, \"label\": predict_model.argmax(axis=1)})\n",
        "    df.to_csv(\"submission_kaggle_dogs_cats.csv\", index=False)\n",
        "    classes = ['cat', 'dog']\n",
        "\n",
        "    for row in range(0, 3):\n",
        "        plt.title(\"label=%s\" % classes[predict_model.argmax(axis=1)[row]])\n",
        "        img = mpimg.imread('/home/dgambali/repos_last/deep_learning/kaggle_dog_cats/face_test/' + self.test_generator.filenames[row])\n",
        "        imgplot = plt.imshow(img)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBenMqXJws92",
        "colab_type": "text"
      },
      "source": [
        "## Using Saved Model\n",
        "\n",
        "Inorder to use our pretrained model and weights we need to load our model. We use \"load_weights\" to load the pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8N2hINgtr_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(self):\n",
        "    print(\"Loading model from disk\")\n",
        "    # load json and create model\n",
        "    json_file = open('%s.json' % self.save_model_name, 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    self.model = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    self.model.load_weights(\"%s.h5\" % self.save_model_name)\n",
        "    print(\"Loaded model from disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvX2hzCJxcK2",
        "colab_type": "text"
      },
      "source": [
        "## Main class to link all methods\n",
        "We define a class called \"dogs_vs_cats\" where we link all our functions and initialize our model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDmMO6hbt4rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class dogs_vs_cats:\n",
        "    def __init__(self):\n",
        "        self.no_of_epochs=1\n",
        "        self.save_model_name='vgg16'\n",
        "        self.no_of_classes=2\n",
        "    load_and_split_data=load_and_split_data\n",
        "    gen_model_using_transfer_learning=gen_model_using_transfer_learning\n",
        "    train_model=train_model\n",
        "    predict_output=predict_output\n",
        "    save_model=save_model\n",
        "    load_model=load_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbRvwhqLxg-0",
        "colab_type": "text"
      },
      "source": [
        "## Methods to Run our Dogs vs Cats Recognizer\n",
        "\n",
        "Here we define two functions \"dogs_vs_cats_from_scratch\" and \"dogs_vs_cats_reuse_saved_model\". we use \"dogs_vs_cats_from_scratch\" function to train and test and \"dogs_vs_cats_reuse_saved_model\" function to reuse the trained model and do inference on test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RB5SGgvbt-JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dogs_vs_cats_from_scratch():\n",
        "    tflearn=dogs_vs_cats()\n",
        "    tflearn.load_and_split_data()\n",
        "    tflearn.gen_model_using_transfer_learning()\n",
        "    tflearn.train_model()\n",
        "    tflearn.predict_output()\n",
        "    tflearn.save_model()\n",
        "\n",
        "def dogs_vs_cats_reuse_saved_model():\n",
        "    tflearn = dogs_vs_cats()\n",
        "    tflearn.load_and_split_data()\n",
        "    tflearn.load_model()\n",
        "    tflearn.predict_output()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUMc-cT4xnzb",
        "colab_type": "text"
      },
      "source": [
        "## Running Our Dogs vs Cats Recognizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONuFwLBLuLZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    dogs_vs_cats_from_scratch()\n",
        "#    dogs_vs_cats_reuse_saved_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEAT_OQ-xu7m",
        "colab_type": "text"
      },
      "source": [
        "# Future scope:\n",
        "\n",
        "*   We can use other networks like Googlenet, Resnet and try experimenting with accuracy.\n",
        "*   We can also extend it for face recognition.\n",
        "\n",
        "\n",
        "> This tutorial is intended to be a public resource. As such, if you see any glaring inaccuracies or if a critical topic is missing, please feel free to point it out or (preferably) submit a pull request to improve the tutorial. Also, we are always looking to improve the scope of this article. For anything feel free to mail us @ colearninglounge@gmail.com\n",
        "\n",
        "\n",
        "> Author of this article is Venkata Durga Rao Gambali. You can follow him on [LinkedIn](https://www.linkedin.com/in/venkata-durga-rao-gambali-24606b79/), [GitHub](https://github.com/venkatadj).\n"
      ]
    }
  ]
}