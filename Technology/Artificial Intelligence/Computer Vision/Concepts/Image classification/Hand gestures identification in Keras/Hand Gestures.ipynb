{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN to recognize sign language\n",
    "\n",
    "#### Have you ever wondered how a speech or hearing impaired person communicates? Yes, sign language. But sign language doesn't exactly fulfils its pupose unless both speaker and listener know it. Well, one can always take up upon learning the sign language but owing to the advent of machine learning, let's learn how to implement a sign language interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='ASL.png'></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post your doubt/feedback/discussion in our FB group unit <a href='https://www.facebook.com/groups/colearninglounge/learning_content/?filter=465390340908184'>here</a> in the appropriate section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "<ul>\n",
    "    <li>Introduction</li>\n",
    "    <li>Import the libraries</li>\n",
    "    <li>Load the dataset</li>\n",
    "    <li>Explore the dataset</li>\n",
    "    <li>Pre-processing the data</li>\n",
    "    <ul>\n",
    "        <li> Read and resize the images</li>\n",
    "        <li> Perform train, validation and test split</li>\n",
    "    </ul>\n",
    "    <li>Building the model</li>\n",
    "    <li>Test the model</li>\n",
    "    <li>Summary</li>\n",
    "    <li>Credits</li>\n",
    "    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this tutorial, we will learn how to do image classification on hand gestures of a sign language using a convolutional neural network, `AlexNet`. We will start by importing and exploring a Kaggle dataset consisting images of hand gestures in american sign language (ASL) for all english alphabets. For simplicity, we will only be considering the first 3 letters of english language 'A', 'B' and 'C' to train, validate and test our network. Second, we will process our data to obtain train, validation and test sets in the format our neural network will be using them. Third, we will build and train an AlexNet to classify the data into the 3 categories. Next, we will interpret the performance of our model by testing it on test data and obtaining an accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries\n",
    "Python provides a variety of libraries to ease out the computational challenges of coding and handle relatively complex problems rather easily. Here we import the essential libraries for hand gesture image classification task.\n",
    "\n",
    "<ul> \n",
    "    <li> <b> Cv2 </b> : This is an open source computer vision (OpenCV) library which provides programming functionality for real-time computer vision. </li>\n",
    "    <li> <b> OS </b> : This library allows you to interface with the Operating System (OS), and provides OS related functionality. </li>\n",
    "    <li> <b> Random </b> : This library is used to generate pseudo random numbers for different distributions. </li>\n",
    "    <li> <b> Numpy </b> : Numerical Python works on an N-dimensional array object and provides basic and complex mathematical functionality for it. </li>\n",
    "    <li> <b> Matplotlib </b> : This library provides data visualization functionality. </li>\n",
    "    <li> <b> Keras </b> : This library provides a convenient way of making neural network based models and uses tensorflow, CNTK or theano at the backend. </li>\n",
    "    <li> <b> Shutil </b> : This library provides functionality to deal with files or collection of files </li>\n",
    "    <li> <b> Warnings </b> : This library specifies how to deal with warnings.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from sklearn import model_selection\n",
    "import random\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from shutil import unpack_archive\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the libraries have been imported successfully, let's move on to loading the data.\n",
    "\n",
    "\n",
    "### Load the dataset\n",
    "\n",
    "You can download the dataset from <a href='https://www.kaggle.com/grassknoted/asl-alphabet#asl_alphabet_test.zip'>here</a>. The dataset contains around 3000 images for each alphabet covering different variations of the images such as different zoom in ratio, different angles and different lighting conditions. A diverse dataset like this one allows a model to be trained efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "\n",
    "For simplicity, we will only be considering 3 classes of the dataset, namely 'A', 'B' and 'C'. First, let's read the data and corresponding labels of the three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['A','B','C']\n",
    "dataX , dataY = [], []\n",
    "for p in paths:\n",
    "    files = os.listdir(p)\n",
    "    for f in files:\n",
    "        image_ = cv2.imread (p+'/'+f)\n",
    "        image = cv2.resize(image_,(224,224))\n",
    "        label = f[0]\n",
    "        dataX.append(image)\n",
    "        dataY.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 9000 images of all the classes, we will first perform an 80-20 split, to make train and test set containing 7200 and 1800 images respectively, then we will again perform an 90-10 split on train data to make train and validation set containing 6480 and 720 images respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(dataX,dataY))\n",
    "train_, test = model_selection.train_test_split(data, test_size=0.2, random_state=1)\n",
    "train, validation = model_selection.train_test_split(data, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have different train, validation and test sets, we will separate the image data and labels in all sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = zip (*train)\n",
    "valX, valY = zip(*validation)\n",
    "testX, testY = zip(*test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the labels into one-hot encoding format and save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainX = list(trainX)\n",
    "trainY = list(trainY)\n",
    "valX = list(valX)\n",
    "valY = list(valY)\n",
    "testX = list(testX)\n",
    "testY = list(testY)\n",
    "trainY = pd.Categorical(trainY).codes\n",
    "valY = pd.Categorical(valY).codes\n",
    "testY = pd.Categorical(testY).codes\n",
    "trainY_ = np_utils.to_categorical(trainY)\n",
    "valY_ = np_utils.to_categorical(valY)\n",
    "testY_ = np_utils.to_categorical(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('trainX.npy', trainX)\n",
    "np.save('trainY.npy', trainY)\n",
    "np.save('valX.npy', valX)\n",
    "np.save('valY.npy', valY)\n",
    "np.save('testX.npy', testX)\n",
    "np.save('testY.npy', testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have used sklearn library function `train_test_split` to split the data which automatically shuffles the data. You can choose to not use the library, as it is done <a href='https://medium.com/free-code-camp/asl-using-alexnet-training-from-scratch-cfec9a8acf84'>here</a>.\n",
    "\n",
    "### Build the model\n",
    "Now that we have data in appropriate format, let's build the CNN model, AlexNet.\n",
    "\n",
    "AlexNet consists of 5 convolutional layers followed by 3 fully connected layers as depicted in the architecture given below.\n",
    "\n",
    "<img src='AlexNet.png'></img>\n",
    "\n",
    "You can dive deep to understand the architecture of an AlexNet <a href='https://medium.com/@smallfishbigsea/a-walk-through-of-alexnet-6cbd137a5637'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.load('trainX.npy')\n",
    "train_Y = np.load('trainY.npy')\n",
    "val_X = np.load('valX.npy')\n",
    "val_Y = np.load('valY.npy')\n",
    "test_X = np.load('testX.npy')\n",
    "test_Y = np.load('testY.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = trainY_\n",
    "val_Y = valY_\n",
    "test_Y = testY_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 6, 6, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1, 1, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 6147      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 32,383,747\n",
      "Trainable params: 32,360,515\n",
      "Non-trainable params: 23,232\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "# 1st Convolutional Layer\n",
    "model.add(Conv2D(filters=96, input_shape=(224,224,3), kernel_size=(11,11),strides=(4,4), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling \n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "model.add(BatchNormalization())\n",
    "# 2nd Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# 3rd Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# 4th Convolutional Layer\n",
    "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# 5th Convolutional Layer\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "model.add(Activation('relu'))\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# Passing it to a dense layer\n",
    "model.add(Flatten())\n",
    "# 1st Dense Layer\n",
    "model.add(Dense(4096, input_shape=(224*224*3,)))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout to prevent overfitting\n",
    "model.add(Dropout(0.4))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# 2nd Dense Layer\n",
    "model.add(Dense(4096))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.6))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# 3rd Dense Layer\n",
    "model.add(Dense(2048))\n",
    "model.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Batch Normalisation\n",
    "model.add(BatchNormalization())\n",
    "# Output Layer\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8100 samples, validate on 900 samples\n",
      "Epoch 1/60\n",
      "8100/8100 [==============================] - 13s 2ms/step - loss: 0.0093 - acc: 0.9974 - val_loss: 0.0171 - val_acc: 0.5611\n",
      "Epoch 2/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0175 - acc: 0.9935 - val_loss: 5.5632e-04 - val_acc: 0.9589\n",
      "Epoch 3/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0099 - acc: 0.9967 - val_loss: 0.0209 - val_acc: 0.5611\n",
      "Epoch 4/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0521 - acc: 0.9869 - val_loss: 0.0279 - val_acc: 0.3789\n",
      "Epoch 5/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0221 - acc: 0.9920 - val_loss: 0.0308 - val_acc: 0.4133\n",
      "Epoch 6/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0260 - acc: 0.9933 - val_loss: 5.2523e-06 - val_acc: 1.0000\n",
      "Epoch 7/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0072 - acc: 0.9980 - val_loss: 4.4847e-06 - val_acc: 1.0000\n",
      "Epoch 8/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0097 - acc: 0.9968 - val_loss: 0.0219 - val_acc: 0.5122\n",
      "Epoch 9/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0159 - acc: 0.9957 - val_loss: 0.0019 - val_acc: 0.9022\n",
      "Epoch 10/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0123 - acc: 0.9953 - val_loss: 0.0010 - val_acc: 0.9411\n",
      "Epoch 11/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0118 - acc: 0.9960 - val_loss: 1.9903e-05 - val_acc: 1.0000\n",
      "Epoch 12/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0059 - acc: 0.9984 - val_loss: 1.9858e-05 - val_acc: 0.9978\n",
      "Epoch 13/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0065 - acc: 0.9972 - val_loss: 2.0648e-06 - val_acc: 1.0000\n",
      "Epoch 14/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0251 - val_acc: 0.4933\n",
      "Epoch 15/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0550 - acc: 0.9841 - val_loss: 0.0101 - val_acc: 0.6978\n",
      "Epoch 16/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0260 - acc: 0.9912 - val_loss: 0.0297 - val_acc: 0.4767\n",
      "Epoch 17/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0298 - acc: 0.9905 - val_loss: 4.3643e-04 - val_acc: 0.9744\n",
      "Epoch 18/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0136 - acc: 0.9959 - val_loss: 4.4571e-04 - val_acc: 0.9589\n",
      "Epoch 19/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0145 - acc: 0.9949 - val_loss: 0.0163 - val_acc: 0.5578\n",
      "Epoch 20/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0225 - acc: 0.9933 - val_loss: 0.0085 - val_acc: 0.7389\n",
      "Epoch 21/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0280 - acc: 0.9898 - val_loss: 0.0219 - val_acc: 0.4811\n",
      "Epoch 22/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0226 - acc: 0.9925 - val_loss: 9.9335e-05 - val_acc: 0.9867\n",
      "Epoch 23/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0157 - acc: 0.9946 - val_loss: 1.6583e-06 - val_acc: 1.0000\n",
      "Epoch 24/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0106 - acc: 0.9963 - val_loss: 4.8214e-06 - val_acc: 1.0000\n",
      "Epoch 25/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.0057 - val_acc: 0.7900\n",
      "Epoch 26/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0180 - acc: 0.9942 - val_loss: 4.1271e-06 - val_acc: 1.0000\n",
      "Epoch 27/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0089 - acc: 0.9969 - val_loss: 9.9629e-06 - val_acc: 1.0000\n",
      "Epoch 28/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0409 - val_acc: 0.3367\n",
      "Epoch 29/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0379 - acc: 0.9889 - val_loss: 1.2978e-04 - val_acc: 0.9911\n",
      "Epoch 30/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0173 - acc: 0.9951 - val_loss: 0.0328 - val_acc: 0.3711\n",
      "Epoch 31/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0275 - acc: 0.9905 - val_loss: 0.0349 - val_acc: 0.3733\n",
      "Epoch 32/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.1062 - acc: 0.9783 - val_loss: 6.1965e-05 - val_acc: 0.9956\n",
      "Epoch 33/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0227 - acc: 0.9920 - val_loss: 0.0011 - val_acc: 0.9267\n",
      "Epoch 34/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0191 - acc: 0.9944 - val_loss: 5.6048e-05 - val_acc: 0.9956\n",
      "Epoch 35/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0129 - acc: 0.9957 - val_loss: 0.0014 - val_acc: 0.9333\n",
      "Epoch 36/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0195 - acc: 0.9937 - val_loss: 0.0060 - val_acc: 0.7111\n",
      "Epoch 37/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0220 - acc: 0.9926 - val_loss: 3.1782e-04 - val_acc: 0.9667\n",
      "Epoch 38/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0141 - acc: 0.9954 - val_loss: 0.0114 - val_acc: 0.6889\n",
      "Epoch 39/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0184 - acc: 0.9940 - val_loss: 1.2674e-05 - val_acc: 1.0000\n",
      "Epoch 40/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 4.6483e-06 - val_acc: 1.0000\n",
      "Epoch 41/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0125 - acc: 0.9964 - val_loss: 8.7750e-05 - val_acc: 0.9922\n",
      "Epoch 42/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0081 - acc: 0.9978 - val_loss: 0.0069 - val_acc: 0.7089\n",
      "Epoch 43/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0336 - acc: 0.9886 - val_loss: 6.7118e-05 - val_acc: 0.9933\n",
      "Epoch 44/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0128 - acc: 0.9949 - val_loss: 0.0064 - val_acc: 0.7544\n",
      "Epoch 45/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0214 - acc: 0.9937 - val_loss: 4.3870e-05 - val_acc: 0.9944\n",
      "Epoch 46/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0148 - acc: 0.9948 - val_loss: 2.9851e-06 - val_acc: 1.0000\n",
      "Epoch 47/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0113 - acc: 0.9962 - val_loss: 2.3662e-06 - val_acc: 1.0000\n",
      "Epoch 48/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 6.0523e-05 - val_acc: 0.9956\n",
      "Epoch 49/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 6.7691e-06 - val_acc: 1.0000\n",
      "Epoch 50/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0221 - val_acc: 0.5244\n",
      "Epoch 51/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0230 - acc: 0.9943 - val_loss: 4.1284e-05 - val_acc: 0.9956\n",
      "Epoch 52/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0110 - acc: 0.9964 - val_loss: 0.0357 - val_acc: 0.4022\n",
      "Epoch 53/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0687 - acc: 0.9811 - val_loss: 0.0024 - val_acc: 0.8756\n",
      "Epoch 54/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0283 - acc: 0.9901 - val_loss: 8.9717e-04 - val_acc: 0.9178\n",
      "Epoch 55/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0179 - acc: 0.9948 - val_loss: 7.2174e-05 - val_acc: 0.9933\n",
      "Epoch 56/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0107 - acc: 0.9965 - val_loss: 2.1996e-04 - val_acc: 0.9789\n",
      "Epoch 57/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0132 - acc: 0.9947 - val_loss: 2.6176e-06 - val_acc: 1.0000\n",
      "Epoch 58/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0077 - val_acc: 0.7578\n",
      "Epoch 59/60\n",
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0129 - acc: 0.9959 - val_loss: 7.6893e-07 - val_acc: 1.0000\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100/8100 [==============================] - 11s 1ms/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0077 - val_acc: 0.7044\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr=0.002)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\"Checkpoint/weights.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "model.fit(train_X/255.0, train_Y, batch_size=32, epochs=60, verbose=1, shuffle=True, validation_data=(val_X/255.0,val_Y/255.0), callbacks=[checkpoint])\n",
    "model.save_weights(\"Weights/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after we have built and validated the model, we save the model weights, and test it. We will quantify the performance of the model with the help of accuracy score.\n",
    "\n",
    "Accuracy is a ratio of correctly classified samples to that of total samples, a high accuracy is desirable in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33944444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "model.load_weights('Checkpoint/weights.58-0.00.hdf5')\n",
    "test_X=np.load(\"testX.npy\")\n",
    "test_Y=np.load(\"testY.npy\")\n",
    "predict_Y = model.predict(test_X)\n",
    "predict_X = [np.argmax(r) for r in predict_Y]\n",
    "acc_score = accuracy_score(test_Y, predict_X)\n",
    "\n",
    "print(\"Accuracy: \" + str(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Voila! You just implemented an AlexNet for image classification. It will be a worthwhile effort to apply other neural network architectures and see how they perform compared to AlexNet. If you are fairly confident of this network, a more useful approach for real world may be to try interpreting sign language in a continuous streaming video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This tutorial is intended to be a public resource. If you see any glaring inaccuracies or a missing critical topic, please feel free to point it out or submit a pull request to improve the tutorial. \n",
    "Also, we are always looking to improve the scope of this article. For any suggestions and feedback, mail us @ colearninglounge@gmail.com\n",
    "### Credits\n",
    "> This article is authored by: <li>Naveksha Sood : Follow her on <a href='https://www.linkedin.com/in/naveksha-sood-8b6824160/'>LinkedIn</a>, <a href='https://medium.com/@navekshasood'>Medium</a> and <a href='https://github.com/search?q=naveksha+sood'>GitHub</a>.</li><li>Vagdevi Kommineni : Follow her on <a href='www.linkedin.com/in/vagdevi-kommineni-427599114'>LinkedIn</a>, <a href='https://medium.com/@vagdevi.k15'>Medium</a>, <a href='https://vagdevik.wordpress.com'>WordPress</a> and <a href='https://github.com/vagdevik'>Github</a>.</li>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
